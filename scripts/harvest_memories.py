#!/usr/bin/env python3
"""
Memory Harvester - ä»ç°æœ‰é¡¹ç›®çš„ CLAUDE.md å’Œ CURRENT_TASK.md ä¸­æ”¶å‰²è®°å¿†

ç”¨æ³•ï¼š
    # æ”¶å‰²æ‰€æœ‰é¡¹ç›®
    python scripts/harvest_memories.py --all

    # æ”¶å‰²æŒ‡å®šé¡¹ç›®
    python scripts/harvest_memories.py --project ~/projects/è·¨å¢ƒ2

    # å¹²è¿è¡Œï¼ˆä¸å†™å…¥ï¼‰
    python scripts/harvest_memories.py --all --dry-run

    # æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯
    python scripts/harvest_memories.py --all --verbose
"""

import argparse
import hashlib
import os
import re
import sys
from datetime import datetime
from pathlib import Path
from typing import Optional
from uuid import UUID, uuid5

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° path
sys.path.insert(0, str(Path(__file__).parent.parent))

# ç¡®ä¿ä½¿ç”¨ Qdrant Server æ¨¡å¼
os.environ.setdefault("QDRANT_URL", "http://127.0.0.1:6333")

from backend.services.search import SearchService

# ç”¨äºç”Ÿæˆå¹‚ç­‰ ID çš„å‘½åç©ºé—´
NAMESPACE_MEMORY_ANCHOR = UUID("6ba7b810-9dad-11d1-80b4-00c04fd430c8")

# é¡¹ç›®æ ¹ç›®å½•
PROJECTS_ROOT = Path.home() / "projects"

# è¦æ‰«æçš„æ–‡ä»¶æ¨¡å¼
MEMORY_FILES = [
    "CLAUDE.md",
    ".claude/CURRENT_TASK.md",
    "CURRENT_TASK.md",
    ".claude/state/progress_*.md",
]

# è¦æ’é™¤çš„ç›®å½•
EXCLUDE_DIRS = [
    "node_modules",
    ".venv",
    "__pycache__",
    ".git",
    ".qdrant",
    ".memos",
]


def generate_note_id(project_id: str, content: str) -> UUID:
    """
    ç”Ÿæˆå¹‚ç­‰çš„ note IDã€‚

    åŸºäºé¡¹ç›® ID å’Œå†…å®¹å“ˆå¸Œç”Ÿæˆï¼Œç¡®ä¿ç›¸åŒå†…å®¹ä¸ä¼šé‡å¤ã€‚
    """
    content_hash = hashlib.sha256(content.encode()).hexdigest()[:16]
    return uuid5(NAMESPACE_MEMORY_ANCHOR, f"{project_id}:{content_hash}")


def extract_project_id(path: Path) -> str:
    """
    ä»è·¯å¾„æå–é¡¹ç›® IDã€‚

    ä¾‹å¦‚ï¼š/Users/baobao/projects/è·¨å¢ƒ2/CLAUDE.md -> kuajing2
    """
    # æ‰¾åˆ° projects ç›®å½•ä¸‹çš„ç¬¬ä¸€çº§ç›®å½•
    parts = path.parts
    try:
        projects_idx = parts.index("projects")
        project_name = parts[projects_idx + 1]
        # å®‰å…¨è¿‡æ»¤ï¼šåªä¿ç•™å­—æ¯æ•°å­—å’Œä¸‹åˆ’çº¿
        safe_name = "".join(c for c in project_name if c.isalnum() or c in ("_", "-"))
        return safe_name or "default"
    except (ValueError, IndexError):
        return "default"


def parse_memory_anchor_blocks(content: str) -> list[dict]:
    """
    è§£æ ```memory-anchor``` ä»£ç å—ã€‚

    è¿”å›ç»“æ„åŒ–çš„è®°å¿†åˆ—è¡¨ã€‚
    """
    import yaml

    pattern = r"```memory-anchor\n(.*?)```"
    blocks = re.findall(pattern, content, re.DOTALL)

    memories = []
    for block in blocks:
        try:
            data = yaml.safe_load(block)
            if data and isinstance(data, dict):
                memories.append({
                    "id": data.get("id"),
                    "type": data.get("type", "note"),
                    "summary": data.get("summary", ""),
                    "details": data.get("details"),
                    "layer": data.get("layer", "fact"),
                    "tags": data.get("tags", []),
                    "source": "memory-anchor-block",
                })
        except Exception:
            continue

    return memories


def extract_decisions_from_markdown(content: str, source_file: str) -> list[dict]:
    """
    ä» Markdown å†…å®¹ä¸­æå–å†³ç­–ã€Bug ä¿®å¤ç­‰è®°å¿†ã€‚

    ä½¿ç”¨å¯å‘å¼è§„åˆ™è¯†åˆ«é‡è¦å†…å®¹ã€‚
    """
    memories = []

    # 1. è§£æ memory-anchor ä»£ç å—ï¼ˆä¼˜å…ˆï¼‰
    memories.extend(parse_memory_anchor_blocks(content))

    # 2. æå–é¡¹ç›®æ¦‚è¿°ï¼ˆç¬¬ä¸€ä¸ª # æ ‡é¢˜ï¼‰
    title_match = re.search(r"^#\s+(.+?)$", content, re.MULTILINE)
    if title_match:
        project_title = title_match.group(1).strip()
        # è·å–æ ‡é¢˜åçš„ç¬¬ä¸€æ®µæè¿°
        desc_match = re.search(r"^#\s+.+?\n\n(.+?)(?=\n\n|\n#|\Z)", content, re.DOTALL)
        if desc_match:
            desc = desc_match.group(1).strip()
            # æ¸…ç† Markdown æ ¼å¼
            desc = re.sub(r"\*\*(.+?)\*\*", r"\1", desc)  # ç§»é™¤åŠ ç²—
            desc = re.sub(r"`(.+?)`", r"\1", desc)  # ç§»é™¤ä»£ç æ ‡è®°
            if len(desc) > 20:
                memories.append({
                    "type": "decision",
                    "summary": f"é¡¹ç›®: {project_title} - {desc[:150]}",
                    "layer": "fact",
                    "source": f"overview:{source_file}",
                })

    # 3. æå–æ¶æ„ä¿¡æ¯ï¼ˆ## Architecture æˆ– ## æ¶æ„ï¼‰
    arch_patterns = [
        r"##\s*(?:Architecture|æ¶æ„|4 Agent æ¶æ„|Domain Architecture)[^\n]*\n(.*?)(?=\n##|\Z)",
        r"##\s*(?:æ ¸å¿ƒä¸šåŠ¡é“¾è·¯|Business Flow)[^\n]*\n(.*?)(?=\n##|\Z)",
    ]
    for pattern in arch_patterns:
        arch_match = re.search(pattern, content, re.DOTALL | re.IGNORECASE)
        if arch_match:
            arch_content = arch_match.group(1).strip()
            # æå–ä»£ç å—ä¸­çš„æ¶æ„å›¾
            code_match = re.search(r"```[^\n]*\n(.*?)```", arch_content, re.DOTALL)
            if code_match:
                arch_diagram = code_match.group(1).strip()
                if len(arch_diagram) > 20:
                    memories.append({
                        "type": "decision",
                        "summary": f"æ¶æ„: {arch_diagram[:200]}",
                        "layer": "fact",
                        "source": f"architecture:{source_file}",
                    })
                    break

    # 4. æå–æœåŠ¡ç«¯å£ä¿¡æ¯
    port_match = re.search(r"##\s*(?:Service Ports|ç«¯å£|Ports)[^\n]*\n(.*?)(?=\n##|\Z)",
                           content, re.DOTALL | re.IGNORECASE)
    if port_match:
        ports_content = port_match.group(1).strip()
        # æå–è¡¨æ ¼ä¸­çš„ç«¯å£ä¿¡æ¯
        port_lines = []
        for line in ports_content.split("\n"):
            if re.search(r"\|\s*\d+\s*\|", line):
                port_lines.append(line.strip())
        if port_lines:
            memories.append({
                "type": "note",
                "summary": f"æœåŠ¡ç«¯å£: {'; '.join(port_lines[:3])}",
                "layer": "fact",
                "source": f"ports:{source_file}",
            })

    # 5. æå–å¯åŠ¨å‘½ä»¤
    cmd_patterns = [
        r"##\s*(?:Development Commands|å¯åŠ¨|Commands|Quick Start)[^\n]*\n(.*?)(?=\n##|\Z)",
        r"##\s*(?:One-Click Start|å¿«é€Ÿå¯åŠ¨)[^\n]*\n(.*?)(?=\n##|\Z)",
    ]
    for pattern in cmd_patterns:
        cmd_match = re.search(pattern, content, re.DOTALL | re.IGNORECASE)
        if cmd_match:
            cmd_content = cmd_match.group(1).strip()
            # æå–ç¬¬ä¸€ä¸ªä»£ç å—
            code_match = re.search(r"```(?:bash|sh)?\n(.*?)```", cmd_content, re.DOTALL)
            if code_match:
                commands = code_match.group(1).strip()
                if len(commands) > 10:
                    memories.append({
                        "type": "note",
                        "summary": f"å¯åŠ¨å‘½ä»¤: {commands[:150]}",
                        "layer": "fact",
                        "source": f"commands:{source_file}",
                    })
                    break

    # 6. æå–æ›¾å›½è—©å¿ƒæ³•ï¼ˆå¦‚æœæœ‰ï¼‰
    doctrine_match = re.search(r"##\s*æ›¾å›½è—©å¿ƒæ³•[^\n]*\n(.*?)(?=\n##|\Z)",
                               content, re.DOTALL)
    if doctrine_match:
        doctrine = doctrine_match.group(1).strip()
        # æå–å…­å­—è¯€
        six_match = re.search(r"\*\*å…­å­—è¯€\*\*[ï¼š:]\s*(.+?)(?=\n|\Z)", doctrine)
        if six_match:
            memories.append({
                "type": "decision",
                "summary": f"å¼€å‘å‡†åˆ™: {six_match.group(1)[:150]}",
                "layer": "fact",
                "source": f"doctrine:{source_file}",
            })

    # 7. æå–å…³é”®æ–‡ä»¶åˆ—è¡¨
    files_match = re.search(r"##\s*(?:å…³é”®æ–‡ä»¶|Key Files|Important Files)[^\n]*\n(.*?)(?=\n##|\Z)",
                            content, re.DOTALL | re.IGNORECASE)
    if files_match:
        files_content = files_match.group(1).strip()
        # æå–åˆ—è¡¨é¡¹
        file_items = re.findall(r"[-*]\s*`([^`]+)`\s*[-â€“]\s*(.+?)(?=\n|$)", files_content)
        if file_items:
            files_summary = "; ".join([f"{f}: {d[:30]}" for f, d in file_items[:5]])
            memories.append({
                "type": "note",
                "summary": f"å…³é”®æ–‡ä»¶: {files_summary[:200]}",
                "layer": "fact",
                "source": f"files:{source_file}",
            })

    # 8. æå–å¾…é…ç½®/ç¯å¢ƒå˜é‡
    env_match = re.search(r"##\s*(?:å¾…é…ç½®|Environment|Configuration)[^\n]*\n(.*?)(?=\n##|\Z)",
                          content, re.DOTALL | re.IGNORECASE)
    if env_match:
        env_content = env_match.group(1).strip()
        # æå– export è¯­å¥
        exports = re.findall(r"export\s+(\w+)=", env_content)
        if exports:
            memories.append({
                "type": "note",
                "summary": f"éœ€è¦é…ç½®: {', '.join(exports[:5])}",
                "layer": "session",
                "source": f"env:{source_file}",
            })

    # 9. æå– CURRENT_TASK.md çš„ä»»åŠ¡çŠ¶æ€
    if "CURRENT_TASK" in source_file or "progress_" in source_file:
        # æå–æ•´ä¸ªæ–‡ä»¶å†…å®¹ä½œä¸ºä»»åŠ¡çŠ¶æ€
        first_lines = "\n".join(content.strip().split("\n")[:10])
        if len(first_lines) > 20:
            memories.append({
                "type": "note",
                "summary": f"å½“å‰ä»»åŠ¡: {first_lines[:200]}",
                "layer": "session",
                "source": f"task:{source_file}",
            })

    return memories


def discover_memory_files(root: Path) -> list[Path]:
    """
    å‘ç°æ‰€æœ‰éœ€è¦æ‰«æçš„è®°å¿†æ–‡ä»¶ã€‚
    """
    files = []

    for pattern in MEMORY_FILES:
        if "*" in pattern:
            # glob æ¨¡å¼
            files.extend(root.glob(pattern))
        else:
            # ç›´æ¥æ–‡ä»¶
            file_path = root / pattern
            if file_path.exists():
                files.append(file_path)

    return files


def discover_projects() -> list[Path]:
    """
    å‘ç°æ‰€æœ‰é¡¹ç›®ç›®å½•ã€‚
    """
    projects = []

    if not PROJECTS_ROOT.exists():
        print(f"è­¦å‘Š: é¡¹ç›®æ ¹ç›®å½•ä¸å­˜åœ¨: {PROJECTS_ROOT}")
        return projects

    for item in PROJECTS_ROOT.iterdir():
        if item.is_dir() and item.name not in EXCLUDE_DIRS:
            # æ£€æŸ¥æ˜¯å¦æœ‰ CLAUDE.md æˆ–å…¶ä»–è®°å¿†æ–‡ä»¶
            has_memory_file = any(
                (item / pattern.split("*")[0]).exists() or list(item.glob(pattern))
                for pattern in MEMORY_FILES
            )
            if has_memory_file:
                projects.append(item)

    return projects


def harvest_project(
    project_path: Path,
    dry_run: bool = False,
    verbose: bool = False,
) -> tuple[int, int]:
    """
    æ”¶å‰²å•ä¸ªé¡¹ç›®çš„è®°å¿†ã€‚

    Returns:
        (å‘ç°çš„è®°å¿†æ•°, å†™å…¥çš„è®°å¿†æ•°)
    """
    project_id = extract_project_id(project_path)
    memory_files = discover_memory_files(project_path)

    if verbose:
        print(f"\nğŸ“‚ é¡¹ç›®: {project_path.name} (ID: {project_id})")
        print(f"   æ‰¾åˆ° {len(memory_files)} ä¸ªè®°å¿†æ–‡ä»¶")

    all_memories = []

    for file_path in memory_files:
        try:
            content = file_path.read_text(encoding="utf-8")
            relative_path = str(file_path.relative_to(project_path))
            memories = extract_decisions_from_markdown(content, relative_path)

            for memory in memories:
                memory["project_id"] = project_id
                memory["file_path"] = relative_path

            all_memories.extend(memories)

            if verbose and memories:
                print(f"   ğŸ“„ {relative_path}: {len(memories)} æ¡è®°å¿†")
        except Exception as e:
            if verbose:
                print(f"   âš ï¸ è¯»å–å¤±è´¥ {file_path}: {e}")

    if dry_run:
        if verbose:
            for m in all_memories:
                print(f"      [{m['type']}] {m['summary'][:80]}...")
        return len(all_memories), 0

    # å†™å…¥ Qdrant
    if not all_memories:
        return 0, 0

    try:
        # ä¸ºæ¯ä¸ªé¡¹ç›®åˆ›å»ºç‹¬ç«‹çš„ SearchService
        os.environ["MCP_MEMORY_PROJECT_ID"] = project_id
        from backend.config import reset_config
        reset_config()  # é‡ç½®é…ç½®ä»¥ä½¿ç”¨æ–°çš„é¡¹ç›® ID

        service = SearchService()

        notes_to_index = []
        for memory in all_memories:
            note_id = generate_note_id(project_id, memory["summary"])
            notes_to_index.append({
                "id": note_id,
                "content": memory["summary"],
                "layer": memory.get("layer", "fact"),
                "category": "event",  # é»˜è®¤ç±»åˆ«
                "is_active": True,
                "confidence": 0.85,  # ä»æ–‡ä»¶æå–çš„é»˜è®¤ç½®ä¿¡åº¦
                "source": memory.get("source", f"harvest:{project_id}"),
                "created_at": datetime.now().isoformat(),
            })

        indexed_count = service.index_notes_batch(notes_to_index)

        if verbose:
            print(f"   âœ… å†™å…¥ {indexed_count} æ¡è®°å¿†åˆ° collection: {service.collection_name}")

        return len(all_memories), indexed_count

    except Exception as e:
        print(f"   âŒ å†™å…¥å¤±è´¥: {e}")
        return len(all_memories), 0


def main():
    parser = argparse.ArgumentParser(
        description="ä»é¡¹ç›®çš„ CLAUDE.md å’Œ CURRENT_TASK.md ä¸­æ”¶å‰²è®°å¿†"
    )
    parser.add_argument(
        "--all",
        action="store_true",
        help="æ”¶å‰²æ‰€æœ‰é¡¹ç›®",
    )
    parser.add_argument(
        "--project",
        type=Path,
        help="æ”¶å‰²æŒ‡å®šé¡¹ç›®",
    )
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="å¹²è¿è¡Œï¼Œä¸å®é™…å†™å…¥",
    )
    parser.add_argument(
        "--verbose", "-v",
        action="store_true",
        help="æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯",
    )

    args = parser.parse_args()

    if not args.all and not args.project:
        parser.print_help()
        sys.exit(1)

    print("ğŸŒ¾ Memory Harvester å¯åŠ¨")
    print(f"   Qdrant URL: {os.environ.get('QDRANT_URL', 'Not set')}")

    if args.dry_run:
        print("   æ¨¡å¼: å¹²è¿è¡Œï¼ˆä¸å†™å…¥ï¼‰")

    total_found = 0
    total_indexed = 0

    if args.all:
        projects = discover_projects()
        print(f"\nå‘ç° {len(projects)} ä¸ªé¡¹ç›®")

        for project in projects:
            found, indexed = harvest_project(
                project,
                dry_run=args.dry_run,
                verbose=args.verbose,
            )
            total_found += found
            total_indexed += indexed
    else:
        project_path = args.project.expanduser().resolve()
        if not project_path.exists():
            print(f"âŒ é¡¹ç›®ä¸å­˜åœ¨: {project_path}")
            sys.exit(1)

        found, indexed = harvest_project(
            project_path,
            dry_run=args.dry_run,
            verbose=args.verbose,
        )
        total_found += found
        total_indexed += indexed

    print(f"\nğŸ“Š æ”¶å‰²å®Œæˆ")
    print(f"   å‘ç°è®°å¿†: {total_found}")
    print(f"   å†™å…¥è®°å¿†: {total_indexed}")


if __name__ == "__main__":
    main()
