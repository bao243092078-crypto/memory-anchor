"""
Memory Anchor MCP Server v2.0 - ä¾› Claude Code ä½¿ç”¨çš„è®°å¿†æ¥å£

åŸºäº docs/MEMORY_STRATEGY.md çš„äº”å±‚è®¤çŸ¥è®°å¿†æ¨¡å‹ï¼š
- L0: identity_schema (è‡ªæˆ‘æ¦‚å¿µ) - æ ¸å¿ƒèº«ä»½ï¼Œä¸‰æ¬¡å®¡æ‰¹
- L1: active_context (å·¥ä½œè®°å¿†) - ä¼šè¯ä¸´æ—¶çŠ¶æ€ï¼Œä¸æŒä¹…åŒ–
- L2: event_log (æƒ…æ™¯è®°å¿†) - å¸¦æ—¶ç©ºæ ‡è®°çš„äº‹ä»¶
- L3: verified_fact (è¯­ä¹‰è®°å¿†) - éªŒè¯è¿‡çš„é•¿æœŸäº‹å®
- L4: operational_knowledge (æŠ€èƒ½å›¾å¼) - æ“ä½œæ€§çŸ¥è¯†

MCP å·¥å…·ï¼š
- search_memory - æœç´¢æ‚£è€…è®°å¿†ï¼ˆL3ï¼‰
- add_memory - æ·»åŠ è®°å¿†ï¼ˆL2/L3 å±‚ï¼‰
- get_constitution - è·å–å®ªæ³•å±‚ï¼ˆL0ï¼‰
- log_event - è®°å½•äº‹ä»¶åˆ°æƒ…æ™¯è®°å¿†ï¼ˆL2ï¼‰
- search_events - æœç´¢äº‹ä»¶æ—¥å¿—ï¼ˆL2ï¼‰
- promote_to_fact - å°†äº‹ä»¶æå‡ä¸ºäº‹å®ï¼ˆL2 â†’ L3ï¼‰
- search_operations - æœç´¢æ“ä½œæ€§çŸ¥è¯† SOP/Workflowï¼ˆL4ï¼‰

ä½¿ç”¨æ–¹å¼ï¼š
1. åœ¨ Claude Code çš„ MCP é…ç½®ä¸­æ·»åŠ æ­¤æœåŠ¡å™¨
2. Claude Code å¯é€šè¿‡ mcp__memory-anchor__* å·¥å…·è®¿é—®è®°å¿†ç³»ç»Ÿ
"""

import asyncio
from typing import Any, Sequence
from uuid import UUID

from mcp.server import Server
from mcp.server.stdio import stdio_server
from mcp.types import (
    Resource,
    TextContent,
    Tool,
)
from pydantic import AnyUrl

from backend.models.checklist import (
    ChecklistBriefingRequest,
    ChecklistItemCreate,
    ChecklistPriority,
    ChecklistScope,
    PlanSyncRequest,
)
from backend.models.constitution_change import (
    ChangeType,
    ConstitutionProposeRequest,
)
from backend.models.note import MemoryLayer, NoteCategory
from backend.services.checklist_service import get_checklist_service
from backend.services.constitution import get_constitution_service
from backend.services.memory import (
    MemoryAddRequest,
    MemorySearchRequest,
    MemoryService,
    MemorySource,
    get_memory_service,
)

# åˆ›å»º MCP Server
server = Server("memory-anchor")


# === Tools ===


@server.list_tools()
async def list_tools() -> list[Tool]:
    """åˆ—å‡ºå¯ç”¨å·¥å…·"""
    return [
        Tool(
            name="search_memory",
            description="""æœç´¢æ‚£è€…è®°å¿†ã€‚

âš ï¸ **å¼ºåˆ¶è°ƒç”¨åœºæ™¯**ï¼šåœ¨å›ç­”ä»»ä½•ä¸ä»¥ä¸‹å†…å®¹ç›¸å…³çš„é—®é¢˜ä¹‹å‰ï¼Œå¿…é¡»å…ˆè°ƒç”¨æ­¤å·¥å…·ï¼š

**æ‚£è€…ç›¸å…³ï¼ˆç…§æŠ¤åœºæ™¯ï¼‰**ï¼š
- æ‚£è€…èº«ä»½ã€å®¶äººã€è”ç³»æ–¹å¼
- å†å²äº‹ä»¶ã€å»è¿‡çš„åœ°æ–¹ã€è§è¿‡çš„äºº
- ç”¨è¯ã€åŒ»ç–—ã€å¥åº·ç›¸å…³
- æ—¥å¸¸ä¹ æƒ¯ã€åå¥½ã€ç¦å¿Œ

**é¡¹ç›®å¼€å‘ç›¸å…³ï¼ˆå¼€å‘åœºæ™¯ï¼‰**ï¼š
- é¡¹ç›®å†å²ã€ä¹‹å‰åšè¿‡ä»€ä¹ˆ
- è®¾è®¡å†³ç­–ã€æ¶æ„é€‰å‹çš„åŸå› 
- Bug ä¿®å¤è®°å½•ã€è¸©è¿‡çš„å‘
- ä¸Šä¸‹æ–‡ã€èƒŒæ™¯ä¿¡æ¯
- "ä¸Šæ¬¡æˆ‘ä»¬è®¨è®ºçš„..."ã€"ä¹‹å‰å†³å®šçš„..."

**æ ¸å¿ƒè§„åˆ™**ï¼šå¦‚æœå½“å‰ä»»åŠ¡ä¸æ˜¯"å®Œå…¨æ–°ä¸œè¥¿"ï¼Œå°±å¿…é¡»å…ˆè°ƒç”¨æ­¤å·¥å…·ã€‚
ä¸ç¡®å®šæ—¶ï¼Œå®å¯å¤šæŸ¥ä¸€æ¬¡ï¼Œä¹Ÿä¸è¦æ¼æ‰é‡è¦ä¸Šä¸‹æ–‡ã€‚

**è¾“å…¥**ï¼šç”¨æˆ·é—®é¢˜çš„ç®€çŸ­æ¦‚è¿°ï¼ˆè‡ªç„¶è¯­è¨€ï¼‰
**è¾“å‡º**ï¼šè‹¥å¹²æ¡ç›¸å…³è®°å¿†ï¼ˆå®ªæ³•/äº‹å®/ä¼šè¯å±‚ï¼‰ï¼Œä¾›ä½ å¼•ç”¨å›ç­”é—®é¢˜

ä¸‰å±‚è®°å¿†è¯´æ˜ï¼š
- ğŸ”´ å®ªæ³•å±‚ï¼šæ ¸å¿ƒèº«ä»½ï¼ˆå§‹ç»ˆè¿”å›ï¼Œä¸å¯é—æ¼ï¼‰
- ğŸ”µ äº‹å®å±‚ï¼šé•¿æœŸè®°å¿†ï¼ˆç»è¿‡éªŒè¯çš„äº‹å®ï¼‰
- ğŸŸ¢ ä¼šè¯å±‚ï¼šçŸ­æœŸå¯¹è¯è®°å¿†ï¼ˆ24hå†…ï¼‰

ç¤ºä¾‹æŸ¥è¯¢ï¼š
- "å¥³å„¿ç”µè¯" â†’ è¿”å›è”ç³»äººä¿¡æ¯
- "search_memory Bug" â†’ è¿”å›ç›¸å…³ Bug ä¿®å¤è®°å½•
- "Qdrant å†³ç­–" â†’ è¿”å›æŠ€æœ¯é€‰å‹åŸå› 
- "ä¸Šæ¬¡è®¨è®ºçš„æ¶æ„" â†’ è¿”å›è®¾è®¡å†³ç­–""",
            inputSchema={
                "type": "object",
                "properties": {
                    "query": {
                        "type": "string",
                        "description": "æœç´¢æŸ¥è¯¢ï¼Œæ”¯æŒè‡ªç„¶è¯­è¨€",
                    },
                    "layer": {
                        "type": "string",
                        "enum": [
                            "identity_schema",
                            "event_log",
                            "verified_fact",
                            "constitution",
                            "fact",
                            "session",
                        ],
                        "description": "è¿‡æ»¤è®°å¿†å±‚çº§ï¼ˆå¯é€‰ï¼‰ã€‚æ–°æœ¯è¯­ï¼šidentity_schema/event_log/verified_factï¼›æ—§æœ¯è¯­ï¼ˆå…¼å®¹ï¼‰ï¼šconstitution/fact/session",
                    },
                    "category": {
                        "type": "string",
                        "enum": ["person", "place", "event", "item", "routine"],
                        "description": "è¿‡æ»¤åˆ†ç±»ï¼ˆå¯é€‰ï¼‰",
                    },
                    "limit": {
                        "type": "integer",
                        "default": 5,
                        "minimum": 1,
                        "maximum": 20,
                        "description": "è¿”å›æ•°é‡é™åˆ¶",
                    },
                },
                "required": ["query"],
            },
        ),
        Tool(
            name="add_memory",
            description="""æ·»åŠ è®°å¿†åˆ°ç³»ç»Ÿã€‚

æ³¨æ„ï¼š
- å®ªæ³•å±‚ä¸å…è®¸é€šè¿‡æ­¤å·¥å…·æ·»åŠ ï¼ˆéœ€ä¸“ç”¨æµç¨‹ï¼‰
- AIæå–çš„è®°å¿†éœ€æä¾›ç½®ä¿¡åº¦ï¼Œä¼šæŒ‰è§„åˆ™å¤„ç†ï¼š
  - â‰¥0.9: ç›´æ¥å­˜å…¥
  - 0.7-0.9: å¾…ç¡®è®¤
  - <0.7: æ‹’ç»

ç¤ºä¾‹ï¼š
- æ·»åŠ æ‚£è€…è‡ªè¿°ï¼š"æ‚£è€…è¯´ä»Šå¤©è§äº†è€æœ‹å‹å¼ ä¸‰"
- è®°å½•è§‚å¯Ÿï¼š"æ‚£è€…è¡¨ç°å‡ºå¯¹èŠ±å›­çš„å–œçˆ±" """,
            inputSchema={
                "type": "object",
                "properties": {
                    "content": {
                        "type": "string",
                        "description": "è®°å¿†å†…å®¹",
                        "minLength": 1,
                        "maxLength": 2000,
                    },
                    "layer": {
                        "type": "string",
                        "enum": [
                            "verified_fact",
                            "event_log",
                            "fact",
                            "session",
                        ],
                        "default": "verified_fact",
                        "description": "è®°å¿†å±‚çº§ã€‚æ¨èï¼šverified_factï¼ˆL3ï¼‰æˆ– event_logï¼ˆL2ï¼‰ã€‚æ—§æœ¯è¯­ fact/session ä»å…¼å®¹ã€‚ä¸å…è®¸ identity_schema/constitution",
                    },
                    "category": {
                        "type": "string",
                        "enum": ["person", "place", "event", "item", "routine"],
                        "description": "åˆ†ç±»ï¼ˆå¯é€‰ï¼‰",
                    },
                    "confidence": {
                        "type": "number",
                        "default": 0.8,
                        "minimum": 0.0,
                        "maximum": 1.0,
                        "description": "ç½®ä¿¡åº¦ï¼ˆAIæå–æ—¶å¿…å¡«ï¼‰",
                    },
                },
                "required": ["content"],
            },
        ),
        Tool(
            name="get_constitution",
            description="""è·å–æ‚£è€…çš„å…¨éƒ¨å®ªæ³•å±‚è®°å¿†ã€‚

å®ªæ³•å±‚åŒ…å«æ‚£è€…çš„æ ¸å¿ƒèº«ä»½ä¿¡æ¯ï¼š
- å§“åã€å¹´é¾„ã€ä½å€
- å…³é”®å®¶åº­æˆå‘˜å’Œè”ç³»æ–¹å¼
- å¿…è¦çš„åŒ»ç–—ä¿¡æ¯ï¼ˆç”¨è¯ã€è¿‡æ•ï¼‰

è¿™äº›ä¿¡æ¯å§‹ç»ˆå…¨é‡è¿”å›ï¼Œä¸ä¾èµ–æ£€ç´¢ã€‚
æ¯æ¬¡å¯¹è¯å¼€å§‹æ—¶åº”è°ƒç”¨æ­¤å·¥å…·åŠ è½½ä¸Šä¸‹æ–‡ã€‚""",
            inputSchema={
                "type": "object",
                "properties": {},
            },
        ),
        Tool(
            name="delete_memory",
            description="""åˆ é™¤æŒ‡å®šçš„è®°å¿†ã€‚

ğŸ”´ **é«˜é£é™©æ“ä½œ** - éœ€è¦ç”¨æˆ·æ˜ç¡®ç¡®è®¤ã€‚

è°ƒç”¨æ­¤å·¥å…·å‰ï¼Œè¯·ç¡®ä¿ï¼š
1. ç”¨æˆ·åœ¨æ¶ˆæ¯ä¸­åŒ…å«ç¡®è®¤çŸ­è¯­ï¼š
   - "ç¡®è®¤åˆ é™¤" / "æˆ‘ç¡®è®¤"
   - "confirm delete" / "I confirm"
2. å·²å‘ç”¨æˆ·è¯´æ˜å°†è¦åˆ é™¤çš„å†…å®¹

å¦‚æœæ²¡æœ‰ç¡®è®¤çŸ­è¯­ï¼Œæ“ä½œå°†è¢«æ‹¦æˆªã€‚

**ä½¿ç”¨åœºæ™¯**ï¼š
- æ¸…ç†é”™è¯¯æ·»åŠ çš„è®°å¿†
- åˆ é™¤è¿‡æ—¶çš„ä¿¡æ¯
- æ‚£è€…/ç…§æŠ¤è€…è¦æ±‚åˆ é™¤

**æ³¨æ„**ï¼š
- ä¸å…è®¸åˆ é™¤å®ªæ³•å±‚è®°å¿†ï¼ˆéœ€ä½¿ç”¨ propose_constitution_changeï¼‰
- åˆ é™¤æ“ä½œä¸å¯é€†
- å»ºè®®å…ˆ search_memory ç¡®è®¤è¦åˆ é™¤çš„å†…å®¹""",
            inputSchema={
                "type": "object",
                "properties": {
                    "note_id": {
                        "type": "string",
                        "description": "è¦åˆ é™¤çš„è®°å¿† IDï¼ˆUUIDï¼‰",
                    },
                    "confirmation": {
                        "type": "string",
                        "description": "ç¡®è®¤çŸ­è¯­ï¼ˆå¿…é¡»åŒ…å« 'ç¡®è®¤åˆ é™¤' æˆ– 'confirm delete'ï¼‰",
                    },
                },
                "required": ["note_id", "confirmation"],
            },
        ),
        Tool(
            name="propose_constitution_change",
            description="""æè®®ä¿®æ”¹å®ªæ³•å±‚è®°å¿†ï¼ˆéœ€ä¸‰æ¬¡å®¡æ‰¹ï¼‰ã€‚

âš ï¸ **å¼ºåˆ¶è§„åˆ™**ï¼šå®ªæ³•å±‚çš„ä»»ä½•ä¿®æ”¹ï¼Œå¿…é¡»é€šè¿‡æ­¤å·¥å…·æè®®ï¼Œä¸å¾—ç›´æ¥ç¼–è¾‘ã€‚

ä¸‰æ¬¡å®¡æ‰¹æµç¨‹ï¼š
1. è°ƒç”¨æ­¤å·¥å…· â†’ åˆ›å»º pending çŠ¶æ€çš„å˜æ›´æè®®
2. ç…§æŠ¤è€…å®¡æ‰¹ 3 æ¬¡ â†’ approvals_count è¾¾åˆ° 3
3. è‡ªåŠ¨åº”ç”¨å˜æ›´ â†’ å†™å…¥å®ªæ³•å±‚

**ä½•æ—¶ä½¿ç”¨**ï¼š
- ä¿®æ”¹æ‚£è€…æ ¸å¿ƒèº«ä»½ï¼ˆå§“åã€ä½å€ï¼‰
- æ›´æ–°è”ç³»äººä¿¡æ¯
- ä¿®æ”¹åŒ»ç–—ä¿¡æ¯ï¼ˆç”¨è¯ã€è¿‡æ•ï¼‰
- åˆ é™¤é”™è¯¯çš„å®ªæ³•å±‚æ¡ç›®

**é‡è¦**ï¼šä»…ç”¨äºæè®®ï¼Œä¸ä¼šç«‹å³ç”Ÿæ•ˆã€‚éœ€è¦ç…§æŠ¤è€…å¤šæ¬¡ç¡®è®¤ã€‚""",
            inputSchema={
                "type": "object",
                "properties": {
                    "change_type": {
                        "type": "string",
                        "enum": ["create", "update", "delete"],
                        "default": "create",
                        "description": "å˜æ›´ç±»å‹ï¼šcreate=æ–°å¢, update=ä¿®æ”¹, delete=åˆ é™¤",
                    },
                    "proposed_content": {
                        "type": "string",
                        "description": "æè®®çš„å†…å®¹ï¼ˆæ–°å¢æˆ–ä¿®æ”¹åçš„å†…å®¹ï¼‰",
                        "minLength": 1,
                        "maxLength": 1000,
                    },
                    "reason": {
                        "type": "string",
                        "description": "å˜æ›´ç†ç”±ï¼ˆå¿…å¡«ï¼Œè¯´æ˜ä¸ºä»€ä¹ˆè¦ä¿®æ”¹ï¼‰",
                        "minLength": 1,
                        "maxLength": 500,
                    },
                    "target_id": {
                        "type": "string",
                        "description": "ç›®æ ‡æ¡ç›®IDï¼ˆupdate/deleteæ—¶å¿…å¡«ï¼‰",
                    },
                    "category": {
                        "type": "string",
                        "enum": ["person", "place", "event", "item", "routine"],
                        "description": "åˆ†ç±»ï¼ˆå¯é€‰ï¼‰",
                    },
                },
                "required": ["proposed_content", "reason"],
            },
        ),
        Tool(
            name="sync_to_files",
            description="""å°† Qdrant ä¸­çš„è®°å¿†åŒæ­¥åˆ° .memos/ æ–‡ä»¶ï¼ˆäººç±»å¯è¯»å¤‡ä»½ï¼‰ã€‚

**ç”¨é€”**ï¼š
- å°† Qdrant ä¸­çš„è®°å¿†å¯¼å‡ºä¸º Markdown æ–‡ä»¶
- ä¾¿äºäººç±»é˜…è¯»å’Œç‰ˆæœ¬æ§åˆ¶
- ä½œä¸º MCP ç¦»çº¿æ—¶çš„å›é€€æ•°æ®æº

**åŒæ­¥ç›®æ ‡**ï¼š
- .memos/fact.md - äº‹å®å±‚è®°å¿†
- .memos/session.md - ä¼šè¯å±‚è®°å¿†
- .memos/index.md - è®°å¿†ç´¢å¼•

**è§¦å‘æ—¶æœº**ï¼š
- ä¼šè¯ç»“æŸæ—¶è‡ªåŠ¨è°ƒç”¨
- ç”¨æˆ·è¯´"åŒæ­¥è®°å¿†"æ—¶æ‰‹åŠ¨è°ƒç”¨""",
            inputSchema={
                "type": "object",
                "properties": {
                    "project_path": {
                        "type": "string",
                        "description": "é¡¹ç›®è·¯å¾„ï¼ˆé»˜è®¤å½“å‰ç›®å½•ï¼‰",
                    },
                    "layers": {
                        "type": "array",
                        "items": {
                            "type": "string",
                            "enum": [
                                "verified_fact",
                                "event_log",
                                "fact",
                                "session",
                            ],
                        },
                        "description": "è¦åŒæ­¥çš„å±‚çº§ï¼ˆé»˜è®¤å…¨éƒ¨ï¼‰ã€‚æ–°æœ¯è¯­ï¼šverified_fact/event_logï¼›æ—§æœ¯è¯­ï¼šfact/session",
                    },
                },
            },
        ),
        # ===== L2 Event Log å·¥å…·ï¼ˆäº”å±‚æ¨¡å‹æ–°å¢ï¼‰=====
        Tool(
            name="log_event",
            description="""è®°å½•äº‹ä»¶åˆ°æƒ…æ™¯è®°å¿†ï¼ˆL2 event_logï¼‰ã€‚

æƒ…æ™¯è®°å¿†çš„æ ¸å¿ƒç‰¹å¾ï¼ˆæ¥è‡ªè®¤çŸ¥ç§‘å­¦ï¼‰ï¼š
- **when**ï¼šäº‹ä»¶å‘ç”Ÿæ—¶é—´
- **where**ï¼šäº‹ä»¶å‘ç”Ÿåœ°ç‚¹
- **who**ï¼šæ¶‰åŠçš„äººç‰©

**ç”¨é€”**ï¼š
- è®°å½•æ‚£è€…çš„æ—¥å¸¸æ´»åŠ¨
- è®°å½•é¡¹ç›®å¼€å‘ä¸­çš„é‡è¦äº‹ä»¶
- è®°å½• Bug ä¿®å¤ã€åŠŸèƒ½å®Œæˆç­‰é‡Œç¨‹ç¢‘

**ä¸ add_memory çš„åŒºåˆ«**ï¼š
- log_event ä¸“é—¨ç”¨äºå¸¦æ—¶ç©ºæ ‡è®°çš„äº‹ä»¶ï¼ˆL2ï¼‰
- add_memory ç”¨äºé€šç”¨è®°å¿†ï¼ˆL3 verified_factï¼‰

**ç¤ºä¾‹**ï¼š
- "æ‚£è€…ä»Šå¤©ä¸‹åˆåœ¨èŠ±å›­æ•£æ­¥ï¼Œé‡åˆ°äº†è€æœ‹å‹å¼ ä¸‰"
- "ä¿®å¤äº† search_memory ç©ºæŸ¥è¯¢çš„ Bug"
- "å®Œæˆäº†äº”å±‚è®°å¿†æ¨¡å‹çš„ MCP å·¥å…·æ·»åŠ " """,
            inputSchema={
                "type": "object",
                "properties": {
                    "content": {
                        "type": "string",
                        "description": "äº‹ä»¶å†…å®¹æè¿°",
                        "minLength": 1,
                        "maxLength": 2000,
                    },
                    "when": {
                        "type": "string",
                        "format": "date-time",
                        "description": "äº‹ä»¶å‘ç”Ÿæ—¶é—´ï¼ˆISO 8601æ ¼å¼ï¼Œé»˜è®¤å½“å‰æ—¶é—´ï¼‰",
                    },
                    "where": {
                        "type": "string",
                        "description": "äº‹ä»¶å‘ç”Ÿåœ°ç‚¹ï¼ˆå¯é€‰ï¼‰",
                        "maxLength": 200,
                    },
                    "who": {
                        "type": "array",
                        "items": {"type": "string"},
                        "description": "æ¶‰åŠçš„äººç‰©åˆ—è¡¨ï¼ˆå¯é€‰ï¼‰",
                    },
                    "category": {
                        "type": "string",
                        "enum": ["person", "place", "event", "item", "routine"],
                        "description": "äº‹ä»¶åˆ†ç±»ï¼ˆå¯é€‰ï¼‰",
                    },
                    "ttl_days": {
                        "type": "integer",
                        "minimum": 1,
                        "description": "å­˜æ´»å¤©æ•°ï¼ˆå¯é€‰ï¼Œé»˜è®¤æ°¸ä¹…ï¼‰",
                    },
                    "confidence": {
                        "type": "number",
                        "default": 0.8,
                        "minimum": 0.0,
                        "maximum": 1.0,
                        "description": "ç½®ä¿¡åº¦",
                    },
                },
                "required": ["content"],
            },
        ),
        Tool(
            name="search_events",
            description="""æœç´¢äº‹ä»¶æ—¥å¿—ï¼ˆL2 event_logï¼‰ã€‚

æ”¯æŒå¤šç»´åº¦è¿‡æ»¤ï¼š
- **æ—¶é—´èŒƒå›´**ï¼šstart_time / end_time
- **åœ°ç‚¹**ï¼šwhere
- **äººç‰©**ï¼šwho
- **è¯­ä¹‰æœç´¢**ï¼šquery

**ä¸ search_memory çš„åŒºåˆ«**ï¼š
- search_events ä¸“é—¨æœç´¢ L2 event_logï¼Œæ”¯æŒæ—¶é—´èŒƒå›´
- search_memory æœç´¢æ‰€æœ‰å±‚çº§çš„è®°å¿†

**ç¤ºä¾‹æŸ¥è¯¢**ï¼š
- æœç´¢ä¸Šå‘¨çš„æ‰€æœ‰äº‹ä»¶
- æœç´¢å‘ç”Ÿåœ¨"èŠ±å›­"çš„äº‹ä»¶
- æœç´¢æ¶‰åŠ"å¼ ä¸‰"çš„äº‹ä»¶""",
            inputSchema={
                "type": "object",
                "properties": {
                    "query": {
                        "type": "string",
                        "description": "æœç´¢æŸ¥è¯¢ï¼ˆå¯é€‰ï¼Œç•™ç©ºè¿”å›æœ€è¿‘äº‹ä»¶ï¼‰",
                    },
                    "start_time": {
                        "type": "string",
                        "format": "date-time",
                        "description": "å¼€å§‹æ—¶é—´ï¼ˆISO 8601æ ¼å¼ï¼‰",
                    },
                    "end_time": {
                        "type": "string",
                        "format": "date-time",
                        "description": "ç»“æŸæ—¶é—´ï¼ˆISO 8601æ ¼å¼ï¼‰",
                    },
                    "where": {
                        "type": "string",
                        "description": "åœ°ç‚¹è¿‡æ»¤",
                    },
                    "who": {
                        "type": "string",
                        "description": "äººç‰©è¿‡æ»¤ï¼ˆæ¶‰åŠæ­¤äººçš„äº‹ä»¶ï¼‰",
                    },
                    "limit": {
                        "type": "integer",
                        "default": 10,
                        "minimum": 1,
                        "maximum": 50,
                        "description": "è¿”å›æ•°é‡é™åˆ¶",
                    },
                },
            },
        ),
        Tool(
            name="promote_to_fact",
            description="""å°†äº‹ä»¶æå‡ä¸ºéªŒè¯äº‹å®ï¼ˆL2 â†’ L3ï¼‰ã€‚

å½“ä¸€ä¸ªäº‹ä»¶ç»è¿‡éªŒè¯ï¼Œå¯ä»¥ä»æƒ…æ™¯è®°å¿†ï¼ˆL2 event_logï¼‰æå‡ä¸ºè¯­ä¹‰è®°å¿†ï¼ˆL3 verified_factï¼‰ã€‚

**ä½•æ—¶ä½¿ç”¨**ï¼š
- äº‹ä»¶è¢«ç…§æŠ¤è€…/ç”¨æˆ·ç¡®è®¤ä¸ºçœŸå®
- ä¸´æ—¶å‘ç°éœ€è¦è½¬ä¸ºé•¿æœŸè®°å¿†
- é‡å¤å‡ºç°çš„äº‹ä»¶éœ€è¦å›ºåŒ–

**æå‡åçš„å˜åŒ–**ï¼š
- ä» event_log å±‚ç§»åŠ¨åˆ° verified_fact å±‚
- ä¸å†å— TTL é™åˆ¶ï¼ˆæ°¸ä¹…ä¿ç•™ï¼‰
- æ ‡è®° verified_by å’Œ promoted_at

**ç¤ºä¾‹**ï¼š
- å°†"æ‚£è€…ä»Šå¤©è®¤å‡ºäº†å¥³å„¿"æå‡ä¸ºäº‹å®
- å°†"å‘ç° Qdrant ä¸æ”¯æŒå¹¶å‘"æå‡ä¸ºé•¿æœŸè®°å½•""",
            inputSchema={
                "type": "object",
                "properties": {
                    "event_id": {
                        "type": "string",
                        "description": "è¦æå‡çš„äº‹ä»¶ IDï¼ˆUUIDï¼‰",
                    },
                    "verified_by": {
                        "type": "string",
                        "default": "caregiver",
                        "description": "éªŒè¯è€…ï¼ˆé»˜è®¤ caregiverï¼‰",
                    },
                    "notes": {
                        "type": "string",
                        "description": "æå‡å¤‡æ³¨ï¼ˆå¯é€‰ï¼‰",
                    },
                },
                "required": ["event_id"],
            },
        ),
        # ===== Checklist å·¥å…·ï¼ˆæ¸…å•é©å‘½ - ä¸ Plan skill ååŒï¼‰=====
        Tool(
            name="get_checklist_briefing",
            description="""è·å–æ¸…å•ç®€æŠ¥ï¼ˆä¼šè¯å¼€å§‹æ—¶è°ƒç”¨ï¼‰ã€‚

**æ ¸å¿ƒç†å¿µ**ï¼ˆæ¥è‡ªã€Šæ¸…å•é©å‘½ã€‹+ ä¸‰æ–¹ AI å¤´è„‘é£æš´ï¼‰ï¼š
- Checklist = æˆ˜ç•¥å±‚ï¼ˆè·¨ä¼šè¯æŒä¹…ï¼‰
- Plan skill = æˆ˜æœ¯å±‚ï¼ˆå•æ¬¡ä»»åŠ¡ï¼‰
- é€šè¿‡ (ma:xxx) ID æœºåˆ¶è¿æ¥ä¸¤è€…

**ä½•æ—¶è°ƒç”¨**ï¼š
- SessionStart æ—¶è·å–å¾…åŠæ¸…å•
- ä¸Šä¸‹æ–‡å‹ç¼©åæ¢å¤å·¥ä½œçŠ¶æ€
- ç”¨æˆ·è¯´"æˆ‘åœ¨åšä»€ä¹ˆæ¥ç€ï¼Ÿ"

**è¿”å›æ ¼å¼**ï¼š
Markdown æ ¼å¼çš„æ¸…å•ç®€æŠ¥ï¼ŒæŒ‰ä¼˜å…ˆçº§åˆ†ç»„ï¼ŒåŒ…å« (ma:xxx) å¼•ç”¨ IDã€‚

**ä¸ Plan skill çš„å…³ç³»**ï¼š
- å…ˆè°ƒç”¨ get_checklist_briefing è·å–æˆ˜ç•¥çº¦æŸ
- å†ä½¿ç”¨ Plan skill ç”Ÿæˆå½“å‰ä»»åŠ¡çš„å…·ä½“æ­¥éª¤
- å®Œæˆåè°ƒç”¨ sync_plan_to_checklist åŒæ­¥çŠ¶æ€""",
            inputSchema={
                "type": "object",
                "properties": {
                    "project_id": {
                        "type": "string",
                        "description": "é¡¹ç›® ID",
                    },
                    "scope": {
                        "type": "string",
                        "enum": ["project", "repo", "global"],
                        "description": "ä½œç”¨åŸŸè¿‡æ»¤ï¼ˆå¯é€‰ï¼‰",
                    },
                    "limit": {
                        "type": "integer",
                        "default": 12,
                        "minimum": 1,
                        "maximum": 50,
                        "description": "è¿”å›æ•°é‡é™åˆ¶",
                    },
                    "include_ids": {
                        "type": "boolean",
                        "default": True,
                        "description": "æ˜¯å¦åŒ…å« (ma:xxx) IDï¼ˆä¾› Plan skill å¼•ç”¨ï¼‰",
                    },
                },
                "required": ["project_id"],
            },
        ),
        Tool(
            name="sync_plan_to_checklist",
            description="""ä» Plan åŒæ­¥æ¸…å•çŠ¶æ€ï¼ˆSessionEnd æ—¶è°ƒç”¨ï¼‰ã€‚

**è§£æ plan.md å†…å®¹**ï¼š
1. æ‰¾åˆ° [x] çš„é¡¹ç›®ï¼Œå¦‚æœæœ‰ (ma:xxx) å¼•ç”¨åˆ™æ ‡è®°å¯¹åº”æ¸…å•é¡¹å®Œæˆ
2. æ‰¾åˆ° @persist æ ‡ç­¾çš„é¡¹ç›®ï¼Œåˆ›å»ºæ–°çš„æ¸…å•é¡¹
3. è¿”å›åŒæ­¥ç»“æœ

**ä½•æ—¶è°ƒç”¨**ï¼š
- SessionEnd æ—¶åŒæ­¥ Plan æ‰§è¡Œç»“æœ
- ç”¨æˆ·è¯´"å­˜è¿›åº¦"ã€"åŒæ­¥æ¸…å•"

**ç¤ºä¾‹ plan.md å†…å®¹**ï¼š
```
- [x] ä¿®å¤ QDRANT_URL é—®é¢˜ (ma:abc12345)
- [ ] å®ç° ChecklistService @persist
- [x] æ·»åŠ  MCP å·¥å…· (ma:def67890)
```

**åŒæ­¥ç»“æœ**ï¼š
- (ma:abc12345) å’Œ (ma:def67890) æ ‡è®°ä¸ºå®Œæˆ
- "å®ç° ChecklistService" åˆ›å»ºä¸ºæ–°æ¸…å•é¡¹""",
            inputSchema={
                "type": "object",
                "properties": {
                    "project_id": {
                        "type": "string",
                        "description": "é¡¹ç›® ID",
                    },
                    "session_id": {
                        "type": "string",
                        "description": "ä¼šè¯ IDï¼ˆç”¨äºæ ‡è®°æ¥æºï¼‰",
                    },
                    "plan_markdown": {
                        "type": "string",
                        "description": "plan.md å†…å®¹",
                    },
                },
                "required": ["project_id", "session_id", "plan_markdown"],
            },
        ),
        Tool(
            name="create_checklist_item",
            description="""åˆ›å»ºæ¸…å•é¡¹ã€‚

ç”¨äºæ‰‹åŠ¨åˆ›å»ºè·¨ä¼šè¯æŒä¹…çš„æ¸…å•é¡¹ã€‚

**ä¸ add_memory çš„åŒºåˆ«**ï¼š
- add_memory: æ·»åŠ è®°å¿†ï¼ˆè¢«åŠ¨å­˜å‚¨ï¼‰
- create_checklist_item: åˆ›å»ºå¾…åŠï¼ˆä¸»åŠ¨è·Ÿè¸ªï¼‰

**ä¼˜å…ˆçº§**ï¼š
- 1 (critical): ğŸ”´ ç´§æ€¥
- 2 (high): ğŸŸ  é«˜ä¼˜
- 3 (normal): ğŸŸ¡ æ™®é€š
- 4 (low): ğŸŸ¢ ä½ä¼˜
- 5 (backlog): âšª å¾…å®š

**ç¤ºä¾‹**ï¼š
- åˆ›å»ºä¸€ä¸ªé«˜ä¼˜å…ˆçº§çš„ Bug ä¿®å¤ä»»åŠ¡
- åˆ›å»ºä¸€ä¸ªæ™®é€šçš„åŠŸèƒ½å¼€å‘ä»»åŠ¡""",
            inputSchema={
                "type": "object",
                "properties": {
                    "project_id": {
                        "type": "string",
                        "description": "é¡¹ç›® ID",
                    },
                    "content": {
                        "type": "string",
                        "description": "æ¸…å•å†…å®¹",
                        "minLength": 1,
                        "maxLength": 500,
                    },
                    "priority": {
                        "type": "integer",
                        "enum": [1, 2, 3, 4, 5],
                        "default": 3,
                        "description": "ä¼˜å…ˆçº§ï¼ˆ1=ç´§æ€¥, 2=é«˜, 3=æ™®é€š, 4=ä½, 5=å¾…å®šï¼‰",
                    },
                    "scope": {
                        "type": "string",
                        "enum": ["project", "repo", "global"],
                        "default": "project",
                        "description": "ä½œç”¨åŸŸ",
                    },
                    "tags": {
                        "type": "array",
                        "items": {"type": "string"},
                        "description": "æ ‡ç­¾åˆ—è¡¨",
                    },
                    "ttl_days": {
                        "type": "integer",
                        "minimum": 1,
                        "description": "å­˜æ´»å¤©æ•°ï¼ˆå¯é€‰ï¼‰",
                    },
                },
                "required": ["project_id", "content"],
            },
        ),
        # ===== L4 Operational Knowledge å·¥å…·ï¼ˆäº”å±‚æ¨¡å‹è¡¥å…¨ï¼‰=====
        Tool(
            name="search_operations",
            description="""æœç´¢ L4 æ“ä½œæ€§çŸ¥è¯†ï¼ˆSOP/Workflowï¼‰ã€‚

âš ï¸ **å¼ºåˆ¶è°ƒç”¨åœºæ™¯**ï¼šé‡åˆ°ä»¥ä¸‹æƒ…å†µæ—¶ï¼Œå¿…é¡»å…ˆè°ƒç”¨æ­¤å·¥å…·æŸ¥æ‰¾ SOPï¼š

**åŸºç¡€è®¾æ–½é—®é¢˜**ï¼š
- Qdrant æœªè¿è¡Œã€502 Bad Gatewayã€QDRANT_URL é”™è¯¯
- MCP è¿æ¥å¤±è´¥ã€è®°å¿†ç³»ç»Ÿæ•…éšœ
- éœ€è¦å¯åŠ¨/é‡å¯æœåŠ¡

**å¼€å‘æµç¨‹é—®é¢˜**ï¼š
- ä¼šè¯å¼€å§‹æ—¶çš„æ ‡å‡†æµç¨‹
- è®°å¿†åŒæ­¥ï¼ˆpending â†’ Qdrantï¼‰
- ä¸Šä¸‹æ–‡æ¢å¤

**æ ¸å¿ƒåŸåˆ™**ï¼š
- L4 æ“ä½œæ€§çŸ¥è¯† = AI çš„"è‚Œè‚‰è®°å¿†"
- é‡åˆ°å·²æœ‰ SOP çš„é—®é¢˜ï¼Œåº”è¯¥æŒ‰ SOP æ‰§è¡Œï¼Œè€Œä¸æ˜¯é‡æ–°æ€è€ƒ
- è¿™ç¬¦åˆåŒ—ææ˜ŸåŸåˆ™ï¼š"ä¸ä¾èµ– AI è‡ªè§‰ï¼ˆè¦æœ‰å¼ºåˆ¶æœºåˆ¶ï¼‰"

**è¾“å…¥**ï¼šé—®é¢˜å…³é”®è¯ï¼ˆå¦‚ "qdrant"ã€"pending"ã€"ä¼šè¯å¼€å§‹"ï¼‰
**è¾“å‡º**ï¼šåŒ¹é…çš„ SOP æ–‡ä»¶è·¯å¾„å’Œç®€è¦è¯´æ˜

**ç¤ºä¾‹æŸ¥è¯¢**ï¼š
- "qdrant" â†’ è¿”å› sop-qdrant-startup.md
- "pending" â†’ è¿”å› sop-memory-sync.md
- "ä¼šè¯å¼€å§‹" â†’ è¿”å› workflow-session-start.md""",
            inputSchema={
                "type": "object",
                "properties": {
                    "query": {
                        "type": "string",
                        "description": "æœç´¢å…³é”®è¯",
                    },
                    "include_content": {
                        "type": "boolean",
                        "default": False,
                        "description": "æ˜¯å¦åŒ…å« SOP æ–‡ä»¶å†…å®¹ï¼ˆé»˜è®¤åªè¿”å›è·¯å¾„å’Œæ‘˜è¦ï¼‰",
                    },
                },
                "required": ["query"],
            },
        ),
        # ===== Memory Refiner å·¥å…·ï¼ˆåŸºäº CoDA ä¸Šä¸‹æ–‡è§£è€¦ï¼‰=====
        Tool(
            name="refine_memory",
            description="""ä½¿ç”¨ LLM ç²¾ç‚¼/å‹ç¼©è®°å¿†ï¼ˆåŸºäº CoDA ä¸Šä¸‹æ–‡è§£è€¦æ€æƒ³ï¼‰ã€‚

**æ ¸å¿ƒæ€æƒ³**ï¼ˆæ¥è‡ª CoDA è®ºæ–‡ï¼‰ï¼š
- åœ¨ç‹¬ç«‹ä¸Šä¸‹æ–‡ä¸­å¤„ç†åŸå§‹è®°å¿†ï¼Œé¿å…æ±¡æŸ“ä¸» Agent çš„ä¸Šä¸‹æ–‡
- ä½¿ç”¨ Observation Masking ç­–ç•¥ï¼šä¿ç•™æœ€è¿‘ N æ¡å®Œæ•´å†…å®¹ï¼Œå‹ç¼©æ›´æ—©çš„
- é€šè¿‡ LLM æå–å…³é”®å†³ç­–ã€Bug ä¿®å¤ã€æ¶æ„é€‰æ‹©ç­‰

**ç”¨é€”**ï¼š
- å½“æœç´¢è¿”å›å¤§é‡è®°å¿†æ—¶ï¼Œç²¾ç‚¼ä¸ºç®€æ´æ‘˜è¦
- èŠ‚çœä¸Šä¸‹æ–‡ tokenï¼Œä¿ç•™å…³é”®ä¿¡æ¯
- ç±»ä¼¼ CoDA Executor çš„ç‹¬ç«‹å¤„ç†æ¨¡å¼

**å·¥ä½œæµç¨‹**ï¼š
1. æ¥æ”¶åŸå§‹è®°å¿†åˆ—è¡¨
2. åº”ç”¨ Observation Maskingï¼ˆæœ€è¿‘ 3 æ¡å®Œæ•´ï¼Œæ›´æ—©çš„å‹ç¼©ï¼‰
3. è°ƒç”¨ LLM ç”Ÿæˆç²¾ç‚¼æ‘˜è¦
4. è¿”å›å‹ç¼©åçš„ç»“æœï¼ˆå«å‹ç¼©æ¯”ï¼‰

**æ³¨æ„**ï¼š
- éœ€è¦é…ç½® LLM API Keyï¼ˆANTHROPIC_API_KEY æˆ– OPENAI_API_KEYï¼‰
- æ—  API Key æ—¶ä½¿ç”¨æœ¬åœ°å›é€€ï¼ˆç®€å•æˆªæ–­ï¼‰
- å¯é€šè¿‡ LLM_ENABLED=false ç¦ç”¨æ­¤åŠŸèƒ½

**ç¤ºä¾‹**ï¼š
- æœç´¢è¿”å› 10 æ¡è®°å¿† â†’ ç²¾ç‚¼ä¸º 500 token æ‘˜è¦
- ç²¾ç‚¼ç„¦ç‚¹ï¼š"key_decisions" / "bugs" / "all\"""",
            inputSchema={
                "type": "object",
                "properties": {
                    "query": {
                        "type": "string",
                        "description": "å½“å‰ç”¨æˆ·æŸ¥è¯¢ï¼ˆç”¨äºç¡®å®šå“ªäº›è®°å¿†æ›´ç›¸å…³ï¼‰",
                    },
                    "memories": {
                        "type": "array",
                        "items": {
                            "type": "object",
                            "properties": {
                                "content": {"type": "string"},
                                "layer": {"type": "string"},
                                "score": {"type": "number"},
                            },
                        },
                        "description": "åŸå§‹è®°å¿†åˆ—è¡¨ï¼ˆæ¥è‡ª search_memoryï¼‰",
                    },
                    "max_output_tokens": {
                        "type": "integer",
                        "default": 500,
                        "minimum": 100,
                        "maximum": 2000,
                        "description": "è¾“å‡ºçš„æœ€å¤§ token æ•°",
                    },
                    "focus": {
                        "type": "string",
                        "enum": ["key_decisions", "bugs", "all"],
                        "default": "all",
                        "description": "ç²¾ç‚¼ç„¦ç‚¹",
                    },
                },
                "required": ["query", "memories"],
            },
        ),
    ]


@server.call_tool()
async def call_tool(name: str, arguments: dict[str, Any]) -> Sequence[TextContent]:
    """æ‰§è¡Œå·¥å…·è°ƒç”¨"""
    service = get_memory_service()

    if name == "search_memory":
        return await _handle_search_memory(service, arguments)
    elif name == "add_memory":
        return await _handle_add_memory(service, arguments)
    elif name == "get_constitution":
        return await _handle_get_constitution(service)
    elif name == "propose_constitution_change":
        return await _handle_propose_constitution_change(arguments)
    elif name == "delete_memory":
        return await _handle_delete_memory(arguments)
    elif name == "sync_to_files":
        return await _handle_sync_to_files(arguments)
    # ===== L2 Event Log å·¥å…·ï¼ˆäº”å±‚æ¨¡å‹æ–°å¢ï¼‰=====
    elif name == "log_event":
        return await _handle_log_event(arguments)
    elif name == "search_events":
        return await _handle_search_events(arguments)
    elif name == "promote_to_fact":
        return await _handle_promote_to_fact(arguments)
    # ===== Checklist å·¥å…·ï¼ˆæ¸…å•é©å‘½ï¼‰=====
    elif name == "get_checklist_briefing":
        return await _handle_get_checklist_briefing(arguments)
    elif name == "sync_plan_to_checklist":
        return await _handle_sync_plan_to_checklist(arguments)
    elif name == "create_checklist_item":
        return await _handle_create_checklist_item(arguments)
    # ===== L4 Operational Knowledge å·¥å…·ï¼ˆäº”å±‚æ¨¡å‹è¡¥å…¨ï¼‰=====
    elif name == "search_operations":
        return await _handle_search_operations(arguments)
    # ===== Memory Refiner å·¥å…·ï¼ˆåŸºäº CoDA ä¸Šä¸‹æ–‡è§£è€¦ï¼‰=====
    elif name == "refine_memory":
        return await _handle_refine_memory(arguments)
    else:
        return [TextContent(type="text", text=f"æœªçŸ¥å·¥å…·: {name}")]


async def _handle_search_memory(
    service: MemoryService, arguments: dict
) -> Sequence[TextContent]:
    """å¤„ç†æœç´¢è®°å¿†è¯·æ±‚"""
    query = arguments.get("query", "")
    layer = arguments.get("layer")
    category = arguments.get("category")
    limit = arguments.get("limit", 5)

    request = MemorySearchRequest(
        query=query,
        layer=MemoryLayer.from_string(layer) if layer else None,
        category=NoteCategory(category) if category else None,
        include_constitution=True,
        limit=limit,
        min_score=0.3,
    )

    results = await service.search_memory(request)

    # æ ¼å¼åŒ–è¾“å‡º
    output_lines = [f"ğŸ” æœç´¢ \"{query}\" è¿”å› {len(results)} æ¡ç»“æœï¼š\n"]

    for i, r in enumerate(results, 1):
        layer_icon = {"constitution": "ğŸ”´", "fact": "ğŸ”µ", "session": "ğŸŸ¢"}.get(
            r.layer.value, "âšª"
        )
        constitution_mark = " [æ ¸å¿ƒ]" if r.is_constitution else ""
        output_lines.append(
            f"{i}. {layer_icon} [{r.layer.value}]{constitution_mark} (ç›¸å…³åº¦: {r.score:.2f})"
        )
        output_lines.append(f"   {r.content}\n")

    return [TextContent(type="text", text="\n".join(output_lines))]


async def _handle_add_memory(
    service: MemoryService, arguments: dict
) -> Sequence[TextContent]:
    """å¤„ç†æ·»åŠ è®°å¿†è¯·æ±‚"""
    content = arguments.get("content", "")
    layer = arguments.get("layer", "verified_fact")  # é»˜è®¤ä½¿ç”¨æ–°æœ¯è¯­
    category = arguments.get("category")
    confidence = arguments.get("confidence", 0.8)

    # æ£€æŸ¥å®ªæ³•å±‚ï¼ˆæ–°æ—§æœ¯è¯­éƒ½è¦é˜»æ­¢ï¼‰
    if layer in ("constitution", "identity_schema"):
        return [
            TextContent(
                type="text",
                text="âŒ é”™è¯¯ï¼šå®ªæ³•å±‚ï¼ˆidentity_schemaï¼‰è®°å¿†ä¸å…è®¸é€šè¿‡æ­¤å·¥å…·æ·»åŠ ã€‚è¯·ä½¿ç”¨ propose_constitution_change å·¥å…·ã€‚",
            )
        ]

    try:
        request = MemoryAddRequest(
            content=content,
            layer=MemoryLayer.from_string(layer),
            category=NoteCategory(category) if category else None,
            source=MemorySource.AI_EXTRACTION,  # MCP è°ƒç”¨è§†ä¸º AI æå–
            confidence=confidence,
        )

        result = await service.add_memory(request)

        status_icon = {
            "saved": "âœ…",
            "pending_approval": "â³",
            "rejected_low_confidence": "âŒ",
        }.get(result["status"], "â“")

        output = f"{status_icon} è®°å¿†æ·»åŠ ç»“æœï¼š\n"
        output += f"- çŠ¶æ€: {result['status']}\n"
        output += f"- å±‚çº§: {result['layer']}\n"
        output += f"- ç½®ä¿¡åº¦: {result['confidence']}\n"

        if result.get("id"):
            output += f"- ID: {result['id']}\n"

        if result.get("requires_approval"):
            output += "- âš ï¸ éœ€è¦ç…§æŠ¤è€…å®¡æ‰¹ç¡®è®¤\n"

        if result.get("reason"):
            output += f"- åŸå› : {result['reason']}\n"

        return [TextContent(type="text", text=output)]

    except ValueError as e:
        return [TextContent(type="text", text=f"âŒ é”™è¯¯ï¼š{str(e)}")]


async def _handle_get_constitution(service: MemoryService) -> Sequence[TextContent]:
    """å¤„ç†è·å–å®ªæ³•å±‚è¯·æ±‚"""
    results = await service.get_constitution()

    if not results:
        return [
            TextContent(
                type="text",
                text="ğŸ“‹ å®ªæ³•å±‚ä¸ºç©ºã€‚è¯·è®©ç…§æŠ¤è€…å…ˆæ·»åŠ æ‚£è€…çš„æ ¸å¿ƒèº«ä»½ä¿¡æ¯ã€‚",
            )
        ]

    output_lines = [f"ğŸ”´ å®ªæ³•å±‚è®°å¿†ï¼ˆå…± {len(results)} æ¡æ ¸å¿ƒä¿¡æ¯ï¼‰ï¼š\n"]

    for i, r in enumerate(results, 1):
        category_name = r.category.value if r.category else "æœªåˆ†ç±»"
        output_lines.append(f"{i}. [{category_name}] {r.content}\n")

    return [TextContent(type="text", text="\n".join(output_lines))]


async def _handle_propose_constitution_change(arguments: dict) -> Sequence[TextContent]:
    """å¤„ç†æè®®å®ªæ³•å±‚å˜æ›´è¯·æ±‚"""

    change_type_str = arguments.get("change_type", "create")
    proposed_content = arguments.get("proposed_content", "")
    reason = arguments.get("reason", "")
    target_id_str = arguments.get("target_id")
    category = arguments.get("category")

    if not proposed_content:
        return [TextContent(type="text", text="âŒ é”™è¯¯ï¼šproposed_content æ˜¯å¿…å¡«é¡¹")]

    if not reason:
        return [TextContent(type="text", text="âŒ é”™è¯¯ï¼šreason æ˜¯å¿…å¡«é¡¹ï¼Œè¯·è¯´æ˜å˜æ›´ç†ç”±")]

    try:
        change_type = ChangeType(change_type_str)
    except ValueError:
        return [TextContent(type="text", text=f"âŒ é”™è¯¯ï¼šæ— æ•ˆçš„ change_type: {change_type_str}")]

    # éªŒè¯ update/delete å¿…é¡»æœ‰ target_id
    if change_type in (ChangeType.UPDATE, ChangeType.DELETE) and not target_id_str:
        return [
            TextContent(
                type="text",
                text=f"âŒ é”™è¯¯ï¼š{change_type.value} æ“ä½œå¿…é¡»æä¾› target_id",
            )
        ]

    try:
        request = ConstitutionProposeRequest(
            change_type=change_type,
            proposed_content=proposed_content,
            reason=reason,
            target_id=UUID(target_id_str) if target_id_str else None,
            category=category,
        )

        constitution_service = get_constitution_service()
        result = await constitution_service.propose(request, proposer="claude-code")

        output = "âœ… å®ªæ³•å˜æ›´æè®®å·²åˆ›å»º\n\n"
        output += "ğŸ“‹ å˜æ›´è¯¦æƒ…ï¼š\n"
        output += f"- ID: {result.id}\n"
        output += f"- ç±»å‹: {result.change_type.value}\n"
        output += f"- å†…å®¹: {result.proposed_content}\n"
        output += f"- ç†ç”±: {result.reason}\n"
        output += f"- çŠ¶æ€: {result.status.value}\n"
        output += f"- å®¡æ‰¹è¿›åº¦: {result.approvals_count}/{result.approvals_needed}\n"
        output += "\n"
        output += "â³ ä¸‹ä¸€æ­¥ï¼šéœ€è¦ç…§æŠ¤è€…å®¡æ‰¹ 3 æ¬¡æ‰èƒ½ç”Ÿæ•ˆã€‚\n"
        output += f"   è°ƒç”¨ POST /api/v1/constitution/approve/{result.id} è¿›è¡Œå®¡æ‰¹ã€‚"

        return [TextContent(type="text", text=output)]

    except ValueError as e:
        return [TextContent(type="text", text=f"âŒ é”™è¯¯ï¼š{str(e)}")]


async def _handle_delete_memory(arguments: dict) -> Sequence[TextContent]:
    """
    å¤„ç†åˆ é™¤è®°å¿†è¯·æ±‚ - é«˜é£é™©æ“ä½œï¼Œéœ€è¦ç”¨æˆ·ç¡®è®¤ã€‚

    ä½¿ç”¨ Gating Hook æœºåˆ¶æ‹¦æˆªå±é™©æ“ä½œã€‚
    """
    from uuid import UUID

    from backend.hooks.gating_hook import gate_operation
    from backend.services.search import get_search_service

    note_id = arguments.get("note_id", "")
    confirmation = arguments.get("confirmation", "")

    # Step 1: éªŒè¯ note_id æ ¼å¼
    if not note_id:
        return [TextContent(type="text", text="âŒ é”™è¯¯ï¼šnote_id æ˜¯å¿…å¡«é¡¹")]

    try:
        note_uuid = UUID(note_id)
    except ValueError:
        return [
            TextContent(type="text", text=f"âŒ é”™è¯¯ï¼šæ— æ•ˆçš„ note_id æ ¼å¼: {note_id}")
        ]

    # Step 2: è°ƒç”¨ Gating Hook æ£€æŸ¥ç¡®è®¤çŸ­è¯­
    gate_result = gate_operation(
        tool_name="delete_memory",
        arguments={"note_id": note_id},
        user_message=confirmation,
    )

    if not gate_result["allowed"]:
        # æ“ä½œè¢«æ‹¦æˆªï¼Œè¿”å›ç¡®è®¤æ¶ˆæ¯
        output = gate_result.get("confirmation_message") or gate_result.get("reason") or "æ“ä½œè¢«æ‹¦æˆª"
        return [TextContent(type="text", text=output)]

    # Step 3: æ‰§è¡Œåˆ é™¤
    try:
        search_service = get_search_service()

        # å…ˆæ£€æŸ¥è®°å¿†æ˜¯å¦å­˜åœ¨
        existing = search_service.get_note(note_uuid)
        if not existing:
            return [
                TextContent(
                    type="text",
                    text=f"âŒ é”™è¯¯ï¼šæœªæ‰¾åˆ° ID ä¸º {note_id} çš„è®°å¿†",
                )
            ]

        # æ£€æŸ¥æ˜¯å¦æ˜¯å®ªæ³•å±‚ï¼ˆç¦æ­¢ç›´æ¥åˆ é™¤ï¼‰
        layer = existing.get("layer", "")
        if layer and layer.lower() in ("constitution", "identity_schema"):
            return [
                TextContent(
                    type="text",
                    text="âŒ é”™è¯¯ï¼šæ— æ³•ç›´æ¥åˆ é™¤å®ªæ³•å±‚è®°å¿†ã€‚è¯·ä½¿ç”¨ propose_constitution_change å·¥å…·æè®®åˆ é™¤ã€‚",
                )
            ]

        # æ‰§è¡Œåˆ é™¤
        success = search_service.delete_note(note_uuid)

        if success:
            content = existing.get("content", "")
            output = "âœ… è®°å¿†å·²åˆ é™¤\n\n"
            output += f"- ID: {note_id}\n"
            output += f"- å†…å®¹: {content[:100]}{'...' if len(content) > 100 else ''}\n"
            output += f"- å±‚çº§: {layer}\n"
            output += "\nâš ï¸ æ­¤æ“ä½œä¸å¯é€†ã€‚"
            return [TextContent(type="text", text=output)]
        else:
            return [TextContent(type="text", text="âŒ åˆ é™¤å¤±è´¥ï¼šæœªçŸ¥é”™è¯¯")]

    except Exception as e:
        return [TextContent(type="text", text=f"âŒ åˆ é™¤å¤±è´¥ï¼š{str(e)}")]


async def _handle_sync_to_files(arguments: dict) -> Sequence[TextContent]:
    """å¤„ç†åŒæ­¥åˆ°æ–‡ä»¶è¯·æ±‚ - å°† Qdrant è®°å¿†å¯¼å‡ºåˆ° .memos/ ç›®å½•"""
    import os
    from datetime import datetime
    from pathlib import Path

    from backend.services.search import get_search_service

    project_path = arguments.get("project_path") or os.getcwd()
    layers = arguments.get("layers") or ["fact", "session"]

    # ç¡®ä¿æ˜¯åˆ—è¡¨
    if isinstance(layers, str):
        layers = [layers]

    memos_dir = Path(project_path) / ".memos"

    try:
        # ç¡®ä¿ .memos ç›®å½•å­˜åœ¨
        memos_dir.mkdir(parents=True, exist_ok=True)

        search_service = get_search_service()
        sync_stats = {"fact": 0, "session": 0}
        all_notes = []

        # è·å–å„å±‚è®°å¿†
        for layer in layers:
            notes = search_service.list_notes(layer=layer, limit=500)
            sync_stats[layer] = len(notes)
            all_notes.extend(notes)

        # åŒæ­¥æ—¶é—´æˆ³
        sync_time = datetime.now().isoformat()

        # å†™å…¥ fact.md
        if "fact" in layers:
            fact_notes = [n for n in all_notes if n.get("layer") == "fact"]
            fact_content = _format_notes_markdown(fact_notes, "äº‹å®å±‚è®°å¿†", sync_time)
            (memos_dir / "fact.md").write_text(fact_content, encoding="utf-8")

        # å†™å…¥ session.md
        if "session" in layers:
            session_notes = [n for n in all_notes if n.get("layer") == "session"]
            session_content = _format_notes_markdown(session_notes, "ä¼šè¯å±‚è®°å¿†", sync_time)
            (memos_dir / "session.md").write_text(session_content, encoding="utf-8")

        # å†™å…¥ index.mdï¼ˆç´¢å¼•ï¼‰
        index_content = _format_index_markdown(all_notes, sync_time)
        (memos_dir / "index.md").write_text(index_content, encoding="utf-8")

        # æ„å»ºè¾“å‡º
        output = "âœ… è®°å¿†åŒæ­¥å®Œæˆ\n\n"
        output += f"ğŸ“‚ ç›®æ ‡ç›®å½•: {memos_dir}\n"
        output += f"â° åŒæ­¥æ—¶é—´: {sync_time}\n\n"
        output += "ğŸ“Š ç»Ÿè®¡:\n"
        for layer in layers:
            output += f"  - {layer}: {sync_stats.get(layer, 0)} æ¡\n"
        output += "\nğŸ“„ ç”Ÿæˆæ–‡ä»¶:\n"
        if "fact" in layers:
            output += "  - fact.md\n"
        if "session" in layers:
            output += "  - session.md\n"
        output += "  - index.md\n"

        return [TextContent(type="text", text=output)]

    except Exception as e:
        return [TextContent(type="text", text=f"âŒ åŒæ­¥å¤±è´¥: {str(e)}")]


# ===== L2 Event Log å¤„ç†å‡½æ•°ï¼ˆäº”å±‚æ¨¡å‹æ–°å¢ï¼‰=====


async def _handle_log_event(arguments: dict) -> Sequence[TextContent]:
    """å¤„ç†è®°å½•äº‹ä»¶è¯·æ±‚ï¼ˆL2 event_logï¼‰"""
    from datetime import datetime

    from backend.core.memory_kernel import get_memory_kernel

    content = arguments.get("content", "")
    when_str = arguments.get("when")
    where = arguments.get("where")
    who = arguments.get("who", [])
    category = arguments.get("category")
    ttl_days = arguments.get("ttl_days")
    confidence = arguments.get("confidence", 0.8)

    if not content:
        return [TextContent(type="text", text="âŒ é”™è¯¯ï¼šcontent æ˜¯å¿…å¡«é¡¹")]

    try:
        # è§£ææ—¶é—´
        when = None
        if when_str:
            try:
                when = datetime.fromisoformat(when_str.replace("Z", "+00:00"))
            except ValueError:
                return [
                    TextContent(
                        type="text", text=f"âŒ é”™è¯¯ï¼šæ— æ•ˆçš„æ—¶é—´æ ¼å¼: {when_str}"
                    )
                ]

        kernel = get_memory_kernel()
        result = kernel.log_event(
            content=content,
            when=when,
            where=where,
            who=who if who else None,
            category=category,
            ttl_days=ttl_days,
            confidence=confidence,
        )

        # æ ¼å¼åŒ–è¾“å‡º
        output = "âœ… äº‹ä»¶å·²è®°å½•åˆ°æƒ…æ™¯è®°å¿†ï¼ˆL2 event_logï¼‰\n\n"
        output += "ğŸ“‹ äº‹ä»¶è¯¦æƒ…ï¼š\n"
        output += f"- ID: {result.get('id', 'N/A')}\n"
        output += f"- å†…å®¹: {content}\n"
        if result.get("when"):
            output += f"- æ—¶é—´: {result['when']}\n"
        if where:
            output += f"- åœ°ç‚¹: {where}\n"
        if who:
            output += f"- äººç‰©: {', '.join(who)}\n"
        if category:
            output += f"- åˆ†ç±»: {category}\n"
        if ttl_days:
            output += f"- TTL: {ttl_days} å¤©\n"
        output += f"- ç½®ä¿¡åº¦: {confidence}\n"

        return [TextContent(type="text", text=output)]

    except Exception as e:
        return [TextContent(type="text", text=f"âŒ è®°å½•äº‹ä»¶å¤±è´¥: {str(e)}")]


async def _handle_search_events(arguments: dict) -> Sequence[TextContent]:
    """å¤„ç†æœç´¢äº‹ä»¶è¯·æ±‚ï¼ˆL2 event_logï¼‰"""
    from datetime import datetime

    from backend.core.memory_kernel import get_memory_kernel

    query = arguments.get("query", "")
    start_time_str = arguments.get("start_time")
    end_time_str = arguments.get("end_time")
    where = arguments.get("where")
    who = arguments.get("who")
    limit = arguments.get("limit", 10)

    try:
        # è§£ææ—¶é—´
        start_time = None
        end_time = None

        if start_time_str:
            try:
                start_time = datetime.fromisoformat(
                    start_time_str.replace("Z", "+00:00")
                )
            except ValueError:
                return [
                    TextContent(
                        type="text", text=f"âŒ é”™è¯¯ï¼šæ— æ•ˆçš„å¼€å§‹æ—¶é—´æ ¼å¼: {start_time_str}"
                    )
                ]

        if end_time_str:
            try:
                end_time = datetime.fromisoformat(end_time_str.replace("Z", "+00:00"))
            except ValueError:
                return [
                    TextContent(
                        type="text", text=f"âŒ é”™è¯¯ï¼šæ— æ•ˆçš„ç»“æŸæ—¶é—´æ ¼å¼: {end_time_str}"
                    )
                ]

        kernel = get_memory_kernel()
        results = kernel.search_events(
            query=query,
            start_time=start_time,
            end_time=end_time,
            where=where,
            who=who,
            limit=limit,
        )

        # æ ¼å¼åŒ–è¾“å‡º
        filter_desc = []
        if query:
            filter_desc.append(f'æŸ¥è¯¢="{query}"')
        if start_time:
            filter_desc.append(f"å¼€å§‹={start_time_str}")
        if end_time:
            filter_desc.append(f"ç»“æŸ={end_time_str}")
        if where:
            filter_desc.append(f"åœ°ç‚¹={where}")
        if who:
            filter_desc.append(f"äººç‰©={who}")

        filter_str = ", ".join(filter_desc) if filter_desc else "æ— è¿‡æ»¤æ¡ä»¶"

        output = f"ğŸ” æœç´¢äº‹ä»¶æ—¥å¿—ï¼ˆ{filter_str}ï¼‰\n"
        output += f"ğŸ“Š æ‰¾åˆ° {len(results)} æ¡ç»“æœï¼š\n\n"

        if not results:
            output += "*æš‚æ— åŒ¹é…çš„äº‹ä»¶*"
        else:
            for i, event in enumerate(results, 1):
                output += f"{i}. ğŸŸ¢ [{event.get('when', 'N/A')}]\n"
                output += f"   {event.get('content', '')}\n"
                if event.get("where"):
                    output += f"   ğŸ“ åœ°ç‚¹: {event['where']}\n"
                if event.get("who"):
                    who_list = event["who"]
                    if isinstance(who_list, list):
                        output += f"   ğŸ‘¤ äººç‰©: {', '.join(who_list)}\n"
                    else:
                        output += f"   ğŸ‘¤ äººç‰©: {who_list}\n"
                output += f"   ID: {event.get('id', 'N/A')}\n"
                output += "\n"

        return [TextContent(type="text", text=output)]

    except Exception as e:
        return [TextContent(type="text", text=f"âŒ æœç´¢äº‹ä»¶å¤±è´¥: {str(e)}")]


async def _handle_promote_to_fact(arguments: dict) -> Sequence[TextContent]:
    """å¤„ç†äº‹ä»¶æå‡è¯·æ±‚ï¼ˆL2 â†’ L3ï¼‰"""
    from backend.core.memory_kernel import get_memory_kernel

    event_id = arguments.get("event_id", "")
    verified_by = arguments.get("verified_by", "caregiver")
    notes = arguments.get("notes")

    if not event_id:
        return [TextContent(type="text", text="âŒ é”™è¯¯ï¼ševent_id æ˜¯å¿…å¡«é¡¹")]

    try:
        kernel = get_memory_kernel()
        result = kernel.promote_event_to_fact(
            event_id=event_id,
            verified_by=verified_by,
            notes=notes,
        )

        # æ ¼å¼åŒ–è¾“å‡º
        output = "âœ… äº‹ä»¶å·²æå‡ä¸ºéªŒè¯äº‹å®ï¼ˆL2 â†’ L3ï¼‰\n\n"
        output += "ğŸ“‹ æå‡è¯¦æƒ…ï¼š\n"
        output += f"- åŸäº‹ä»¶ ID: {event_id}\n"
        output += f"- æ–°äº‹å® ID: {result.get('fact_id', 'N/A')}\n"
        output += f"- éªŒè¯è€…: {verified_by}\n"
        if notes:
            output += f"- å¤‡æ³¨: {notes}\n"
        output += f"- æå‡æ—¶é—´: {result.get('promoted_at', 'N/A')}\n"
        output += "\n"
        output += "ğŸ“ æå‡åçš„å˜åŒ–ï¼š\n"
        output += "- ä» event_log å±‚ç§»åŠ¨åˆ° verified_fact å±‚\n"
        output += "- ä¸å†å— TTL é™åˆ¶ï¼ˆæ°¸ä¹…ä¿ç•™ï¼‰\n"

        return [TextContent(type="text", text=output)]

    except ValueError as e:
        return [TextContent(type="text", text=f"âŒ é”™è¯¯ï¼š{str(e)}")]
    except Exception as e:
        return [TextContent(type="text", text=f"âŒ æå‡äº‹ä»¶å¤±è´¥: {str(e)}")]


# ===== Checklist å¤„ç†å‡½æ•°ï¼ˆæ¸…å•é©å‘½ï¼‰=====


async def _handle_get_checklist_briefing(arguments: dict) -> Sequence[TextContent]:
    """å¤„ç†è·å–æ¸…å•ç®€æŠ¥è¯·æ±‚"""
    project_id = arguments.get("project_id", "")
    scope_str = arguments.get("scope")
    limit = arguments.get("limit", 12)
    include_ids = arguments.get("include_ids", True)

    if not project_id:
        return [TextContent(type="text", text="âŒ é”™è¯¯ï¼šproject_id æ˜¯å¿…å¡«é¡¹")]

    try:
        scope = ChecklistScope(scope_str) if scope_str else None

        request = ChecklistBriefingRequest(
            project_id=project_id,
            scope=scope,
            limit=limit,
            include_ids=include_ids,
        )

        service = get_checklist_service()
        briefing = service.get_briefing(request)

        return [TextContent(type="text", text=briefing)]

    except Exception as e:
        return [TextContent(type="text", text=f"âŒ è·å–æ¸…å•ç®€æŠ¥å¤±è´¥: {str(e)}")]


async def _handle_sync_plan_to_checklist(arguments: dict) -> Sequence[TextContent]:
    """å¤„ç† Plan åŒæ­¥è¯·æ±‚"""
    project_id = arguments.get("project_id", "")
    session_id = arguments.get("session_id", "")
    plan_markdown = arguments.get("plan_markdown", "")

    if not project_id:
        return [TextContent(type="text", text="âŒ é”™è¯¯ï¼šproject_id æ˜¯å¿…å¡«é¡¹")]
    if not session_id:
        return [TextContent(type="text", text="âŒ é”™è¯¯ï¼šsession_id æ˜¯å¿…å¡«é¡¹")]
    if not plan_markdown:
        return [TextContent(type="text", text="âŒ é”™è¯¯ï¼šplan_markdown æ˜¯å¿…å¡«é¡¹")]

    try:
        request = PlanSyncRequest(
            project_id=project_id,
            session_id=session_id,
            plan_markdown=plan_markdown,
        )

        service = get_checklist_service()
        result = service.sync_from_plan(request)

        # æ ¼å¼åŒ–è¾“å‡º
        output = "âœ… Plan åŒæ­¥å®Œæˆ\n\n"

        if result.completed:
            output += f"ğŸ“‹ æ ‡è®°å®Œæˆ ({len(result.completed)} é¡¹):\n"
            for short_id in result.completed:
                output += f"  - (ma:{short_id}) âœ“\n"
            output += "\n"

        if result.created:
            output += f"ğŸ“ æ–°å»ºæ¸…å•é¡¹ ({len(result.created)} é¡¹):\n"
            for item in result.created:
                output += f"  - {item.content} {item.ma_ref}\n"
            output += "\n"

        if result.warnings:
            output += "âš ï¸ è­¦å‘Š:\n"
            for warning in result.warnings:
                output += f"  - {warning}\n"
            output += "\n"

        if not result.completed and not result.created:
            output += "*æ— å˜æ›´*\n"

        return [TextContent(type="text", text=output)]

    except Exception as e:
        return [TextContent(type="text", text=f"âŒ Plan åŒæ­¥å¤±è´¥: {str(e)}")]


async def _handle_create_checklist_item(arguments: dict) -> Sequence[TextContent]:
    """å¤„ç†åˆ›å»ºæ¸…å•é¡¹è¯·æ±‚"""
    project_id = arguments.get("project_id", "")
    content = arguments.get("content", "")
    priority_int = arguments.get("priority", 3)
    scope_str = arguments.get("scope", "project")
    tags = arguments.get("tags", [])
    ttl_days = arguments.get("ttl_days")

    if not project_id:
        return [TextContent(type="text", text="âŒ é”™è¯¯ï¼šproject_id æ˜¯å¿…å¡«é¡¹")]
    if not content:
        return [TextContent(type="text", text="âŒ é”™è¯¯ï¼šcontent æ˜¯å¿…å¡«é¡¹")]

    try:
        request = ChecklistItemCreate(
            content=content,
            priority=ChecklistPriority(priority_int),
            scope=ChecklistScope(scope_str),
            tags=tags if tags else [],
            ttl_days=ttl_days,
        )

        service = get_checklist_service()
        item = service.create_item(project_id, request)

        # æ ¼å¼åŒ–è¾“å‡º
        priority_labels = {
            ChecklistPriority.CRITICAL: "ğŸ”´ ç´§æ€¥",
            ChecklistPriority.HIGH: "ğŸŸ  é«˜ä¼˜",
            ChecklistPriority.NORMAL: "ğŸŸ¡ æ™®é€š",
            ChecklistPriority.LOW: "ğŸŸ¢ ä½ä¼˜",
            ChecklistPriority.BACKLOG: "âšª å¾…å®š",
        }

        output = "âœ… æ¸…å•é¡¹å·²åˆ›å»º\n\n"
        output += "ğŸ“‹ è¯¦æƒ…:\n"
        output += f"- å†…å®¹: {item.content}\n"
        output += f"- ID: {item.id}\n"
        output += f"- å¼•ç”¨: {item.ma_ref()}\n"
        output += f"- ä¼˜å…ˆçº§: {priority_labels.get(item.priority, 'æœªçŸ¥')}\n"
        output += f"- ä½œç”¨åŸŸ: {item.scope.value}\n"
        if item.tags:
            output += f"- æ ‡ç­¾: {', '.join(item.tags)}\n"
        if item.expires_at:
            output += f"- è¿‡æœŸæ—¶é—´: {item.expires_at.isoformat()}\n"

        return [TextContent(type="text", text=output)]

    except Exception as e:
        return [TextContent(type="text", text=f"âŒ åˆ›å»ºæ¸…å•é¡¹å¤±è´¥: {str(e)}")]


def _format_notes_markdown(notes: list, title: str, sync_time: str) -> str:
    """æ ¼å¼åŒ–è®°å¿†ä¸º Markdown"""
    lines = [
        f"# {title}",
        "",
        f"> åŒæ­¥æ—¶é—´: {sync_time}",
        f"> è®°å½•æ•°: {len(notes)}",
        "",
        "---",
        "",
    ]

    if not notes:
        lines.append("*æš‚æ— è®°å½•*")
        return "\n".join(lines)

    # æŒ‰ç±»åˆ«åˆ†ç»„
    by_category: dict = {}
    for note in notes:
        cat = note.get("category") or "æœªåˆ†ç±»"
        if cat not in by_category:
            by_category[cat] = []
        by_category[cat].append(note)

    for category, cat_notes in sorted(by_category.items()):
        lines.append(f"## {category}")
        lines.append("")
        for note in cat_notes:
            content = note.get("content", "")
            confidence = note.get("confidence")
            source = note.get("source")
            created_at = note.get("created_at", "")

            lines.append(f"- {content}")
            meta_parts = []
            if confidence:
                meta_parts.append(f"ç½®ä¿¡åº¦: {confidence:.2f}")
            if source:
                meta_parts.append(f"æ¥æº: {source}")
            if created_at:
                meta_parts.append(f"åˆ›å»º: {created_at[:10]}")
            if meta_parts:
                lines.append(f"  - *{' | '.join(meta_parts)}*")
            lines.append("")
        lines.append("")

    return "\n".join(lines)


def _format_index_markdown(notes: list, sync_time: str) -> str:
    """æ ¼å¼åŒ–è®°å¿†ç´¢å¼•"""
    lines = [
        "# Memory Anchor ç´¢å¼•",
        "",
        f"> åŒæ­¥æ—¶é—´: {sync_time}",
        "",
        "---",
        "",
        "## ç»Ÿè®¡",
        "",
    ]

    # ç»Ÿè®¡
    layer_count: dict = {}
    category_count: dict = {}
    for note in notes:
        layer = note.get("layer") or "unknown"
        category = note.get("category") or "æœªåˆ†ç±»"
        layer_count[layer] = layer_count.get(layer, 0) + 1
        category_count[category] = category_count.get(category, 0) + 1

    lines.append("### æŒ‰å±‚çº§")
    lines.append("")
    for layer, count in sorted(layer_count.items()):
        icon = {"constitution": "ğŸ”´", "fact": "ğŸ”µ", "session": "ğŸŸ¢"}.get(layer, "âšª")
        lines.append(f"- {icon} {layer}: {count} æ¡")
    lines.append("")

    lines.append("### æŒ‰ç±»åˆ«")
    lines.append("")
    for category, count in sorted(category_count.items()):
        lines.append(f"- {category}: {count} æ¡")
    lines.append("")

    lines.append("## æ–‡ä»¶")
    lines.append("")
    lines.append("- [fact.md](./fact.md) - äº‹å®å±‚è®°å¿†")
    lines.append("- [session.md](./session.md) - ä¼šè¯å±‚è®°å¿†")
    lines.append("")

    return "\n".join(lines)


# ===== L4 Operational Knowledge Handler =====


async def _handle_search_operations(arguments: dict) -> Sequence[TextContent]:
    """å¤„ç† L4 æ“ä½œæ€§çŸ¥è¯†æœç´¢è¯·æ±‚"""
    import os
    from pathlib import Path

    import yaml

    query = arguments.get("query", "").lower()
    include_content = arguments.get("include_content", False)

    if not query:
        return [TextContent(type="text", text="âŒ é”™è¯¯ï¼šquery æ˜¯å¿…å¡«é¡¹")]

    # è·å–é¡¹ç›®æ ¹ç›®å½•ï¼ˆä»ç¯å¢ƒå˜é‡æˆ–å½“å‰å·¥ä½œç›®å½•ï¼‰
    project_root = os.environ.get("MCP_MEMORY_PROJECT_ROOT")
    if not project_root:
        # å°è¯•ä»å½“å‰æ–‡ä»¶ä½ç½®æ¨æ–­
        current_file = Path(__file__)
        project_root = str(current_file.parent.parent)

    ops_dir = Path(project_root) / ".ai" / "operations"
    index_file = ops_dir / "index.yaml"

    # æ£€æŸ¥ç›®å½•æ˜¯å¦å­˜åœ¨
    if not ops_dir.exists():
        return [
            TextContent(
                type="text",
                text="âš ï¸ L4 æ“ä½œæ€§çŸ¥è¯†ç›®å½•ä¸å­˜åœ¨ã€‚\n\n"
                "è¯·å…ˆåˆ›å»º `.ai/operations/` ç›®å½•å’Œ `index.yaml` ç´¢å¼•æ–‡ä»¶ã€‚\n"
                "å‚è€ƒï¼šdocs/MEMORY_STRATEGY.md çš„ L4 ç« èŠ‚ã€‚",
            )
        ]

    if not index_file.exists():
        return [
            TextContent(
                type="text",
                text="âš ï¸ L4 ç´¢å¼•æ–‡ä»¶ä¸å­˜åœ¨ã€‚\n\n"
                f"è¯·åœ¨ {ops_dir} ç›®å½•ä¸‹åˆ›å»º `index.yaml` æ–‡ä»¶ã€‚",
            )
        ]

    # åŠ è½½ç´¢å¼•
    try:
        with open(index_file, encoding="utf-8") as f:
            index = yaml.safe_load(f)
    except Exception as e:
        return [TextContent(type="text", text=f"âŒ åŠ è½½ç´¢å¼•å¤±è´¥ï¼š{e}")]

    matched_files: list[dict] = []

    # 1. å¿«é€ŸåŒ¹é…ï¼šç›´æ¥å…³é”®è¯åŒ¹é…
    quick_match = index.get("quick_match", {})
    for keyword, files in quick_match.items():
        if query in keyword.lower():
            for file in files:
                file_path = ops_dir / file
                if file_path.exists():
                    matched_files.append(
                        {
                            "file": file,
                            "path": str(file_path),
                            "match_type": "quick_match",
                            "keyword": keyword,
                        }
                    )

    # 2. è§¦å‘æ¡ä»¶åŒ¹é…ï¼šæœç´¢ sops.*.triggers
    for category, sops in index.get("sops", {}).items():
        for sop in sops:
            triggers = sop.get("triggers", [])
            for trigger in triggers:
                if query in trigger.lower():
                    file = sop.get("file", "")
                    file_path = ops_dir / file
                    # é¿å…é‡å¤æ·»åŠ 
                    if not any(m["file"] == file for m in matched_files):
                        if file_path.exists():
                            matched_files.append(
                                {
                                    "file": file,
                                    "path": str(file_path),
                                    "match_type": "trigger",
                                    "trigger": trigger,
                                    "description": sop.get("description", ""),
                                    "category": category,
                                }
                            )
                    break

    # æ ¼å¼åŒ–è¾“å‡º
    if not matched_files:
        return [
            TextContent(
                type="text",
                text=f"ğŸ” æœªæ‰¾åˆ°ä¸ \"{query}\" åŒ¹é…çš„ SOP/Workflowã€‚\n\n"
                "å¯å°è¯•æ›´é€šç”¨çš„å…³é”®è¯ï¼Œæˆ–ç›´æ¥æµè§ˆ `.ai/operations/` ç›®å½•ã€‚",
            )
        ]

    output_lines = [f"âšª L4 æœç´¢ \"{query}\" æ‰¾åˆ° {len(matched_files)} ä¸ªåŒ¹é…ï¼š\n"]

    for i, match in enumerate(matched_files, 1):
        file = match["file"]
        match_type = match["match_type"]
        description = match.get("description", "")
        trigger = match.get("trigger", "")
        keyword = match.get("keyword", "")

        output_lines.append(f"## {i}. {file}")
        if description:
            output_lines.append(f"   ğŸ“‹ {description}")
        if match_type == "quick_match":
            output_lines.append(f"   ğŸ”‘ å¿«é€ŸåŒ¹é…: \"{keyword}\"")
        elif match_type == "trigger":
            output_lines.append(f"   ğŸ¯ è§¦å‘æ¡ä»¶: \"{trigger}\"")
        output_lines.append(f"   ğŸ“ è·¯å¾„: {match['path']}")

        # å¦‚æœéœ€è¦åŒ…å«å†…å®¹
        if include_content:
            file_path = Path(match["path"])
            try:
                content = file_path.read_text(encoding="utf-8")
                # åªå–å‰ 2000 å­—ç¬¦ï¼Œé¿å…è¾“å‡ºè¿‡é•¿
                if len(content) > 2000:
                    content = content[:2000] + "\n\n... (å†…å®¹å·²æˆªæ–­ï¼Œè¯·ç›´æ¥è¯»å–æ–‡ä»¶)"
                output_lines.append("\n```markdown")
                output_lines.append(content)
                output_lines.append("```\n")
            except Exception as e:
                output_lines.append(f"   âš ï¸ è¯»å–å¤±è´¥: {e}")

        output_lines.append("")

    output_lines.append("---")
    output_lines.append("ğŸ’¡ æç¤ºï¼šæŒ‰ SOP æ­¥éª¤æ‰§è¡Œï¼Œè€Œéé‡æ–°æ€è€ƒè§£å†³æ–¹æ¡ˆã€‚")

    return [TextContent(type="text", text="\n".join(output_lines))]


async def _handle_refine_memory(arguments: dict[str, Any]) -> list[TextContent]:
    """
    å¤„ç† refine_memory è°ƒç”¨ - ä½¿ç”¨ LLM ç²¾ç‚¼/å‹ç¼©è®°å¿†

    åŸºäº CoDA ä¸Šä¸‹æ–‡è§£è€¦æ€æƒ³ï¼šåœ¨ç‹¬ç«‹ä¸Šä¸‹æ–‡ä¸­å¤„ç†åŸå§‹è®°å¿†ï¼Œ
    é¿å…æ±¡æŸ“ä¸» Agent çš„ä¸Šä¸‹æ–‡çª—å£ã€‚
    """
    from backend.config import get_config
    from backend.services.memory_refiner import get_memory_refiner

    query = arguments.get("query", "")
    memories = arguments.get("memories", [])
    max_output_tokens = arguments.get("max_output_tokens", 500)
    focus = arguments.get("focus", "key_decisions")

    # æ£€æŸ¥ LLM æ˜¯å¦å¯ç”¨
    config = get_config()
    if not config.llm_enabled:
        return [
            TextContent(
                type="text",
                text="âš ï¸ LLM ç²¾ç‚¼åŠŸèƒ½å·²ç¦ç”¨ï¼ˆLLM_ENABLED=falseï¼‰ã€‚\n"
                "å¦‚éœ€å¯ç”¨ï¼Œè¯·è®¾ç½®ç¯å¢ƒå˜é‡ LLM_ENABLED=true",
            )
        ]

    # æ£€æŸ¥è¾“å…¥
    if not memories:
        return [
            TextContent(
                type="text",
                text="âš ï¸ æœªæä¾›ä»»ä½•è®°å¿†å†…å®¹ã€‚è¯·ä¼ å…¥ memories æ•°ç»„ã€‚",
            )
        ]

    # è°ƒç”¨ Memory Refiner
    try:
        refiner = get_memory_refiner()
        result = await refiner.refine(
            query=query,
            memories=memories,
            max_output_tokens=max_output_tokens,
            focus=focus,
        )

        if not result.success:
            return [
                TextContent(
                    type="text",
                    text=f"âŒ è®°å¿†ç²¾ç‚¼å¤±è´¥: {result.error}\n\n"
                    f"ä½¿ç”¨çš„æ¨¡å‹: {result.llm_model}",
                )
            ]

        # æ ¼å¼åŒ–è¾“å‡º
        output_lines = [
            "âœ¨ **è®°å¿†ç²¾ç‚¼å®Œæˆ**",
            "",
            "ğŸ“Š å‹ç¼©ç»Ÿè®¡:",
            f"   - åŸå§‹è®°å¿†æ•°: {result.original_count}",
            f"   - åŸå§‹ Token (ä¼°): {result.original_tokens}",
            f"   - ç²¾ç‚¼å Token (ä¼°): {result.refined_tokens}",
            f"   - å‹ç¼©æ¯”: {result.compression_ratio:.1%}",
            f"   - ä½¿ç”¨æ¨¡å‹: {result.llm_model}",
            "",
            "---",
            "",
            "ğŸ“ **ç²¾ç‚¼åçš„å†…å®¹:**",
            "",
            result.refined_content,
        ]

        return [TextContent(type="text", text="\n".join(output_lines))]

    except Exception as e:
        # MCP ç›´æ¥è¿”å›é”™è¯¯ç»™å®¢æˆ·ç«¯ï¼Œæ— éœ€é¢å¤–æ—¥å¿—
        return [
            TextContent(
                type="text",
                text=f"âŒ è®°å¿†ç²¾ç‚¼å‘ç”Ÿé”™è¯¯: {str(e)}",
            )
        ]


# === Resources ===


@server.list_resources()
async def list_resources() -> list[Resource]:
    """åˆ—å‡ºå¯ç”¨èµ„æº"""
    return [
        Resource(
            uri=AnyUrl("memory://constitution"),
            name="æ‚£è€…å®ªæ³•å±‚è®°å¿†",
            description="æ‚£è€…çš„æ ¸å¿ƒèº«ä»½ä¿¡æ¯ï¼ŒåŒ…æ‹¬å§“åã€å®¶äººã€ç”¨è¯ç­‰",
            mimeType="text/plain",
        ),
        Resource(
            uri=AnyUrl("memory://recent"),
            name="æœ€è¿‘è®°å¿†",
            description="æœ€è¿‘æ·»åŠ çš„è®°å¿†ï¼ˆä¼šè¯å±‚ + è¿‘æœŸäº‹å®å±‚ï¼‰",
            mimeType="text/plain",
        ),
    ]


@server.read_resource()
async def read_resource(uri: str) -> str:
    """è¯»å–èµ„æºå†…å®¹"""
    service = get_memory_service()

    if uri == "memory://constitution":
        results = await service.get_constitution()
        if not results:
            return "å®ªæ³•å±‚ä¸ºç©º"
        return "\n".join([f"- {r.content}" for r in results])

    elif uri == "memory://recent":
        # æœç´¢æœ€è¿‘çš„è®°å¿†ï¼ˆä½¿ç”¨é€šç”¨å…³é”®è¯æœç´¢å…¨éƒ¨ï¼‰
        request = MemorySearchRequest(
            query="è®°å¿†",  # ä½¿ç”¨é€šç”¨å…³é”®è¯
            include_constitution=False,
            limit=10,
            min_score=0.0,  # ä¸è¿‡æ»¤åˆ†æ•°ï¼Œè¿”å›æ‰€æœ‰åŒ¹é…
        )
        results = await service.search_memory(request)
        if not results:
            return "æš‚æ— æœ€è¿‘è®°å¿†"
        return "\n".join([f"[{r.layer.value}] {r.content}" for r in results])

    return f"æœªçŸ¥èµ„æº: {uri}"


# === Main ===


async def main():
    """å¯åŠ¨ MCP Server"""
    # é‡ç½®æ‰€æœ‰å•ä¾‹ä»¥ç¡®ä¿ä½¿ç”¨æœ€æ–°çš„ç¯å¢ƒå˜é‡ï¼ˆMCP_MEMORY_PROJECT_IDï¼‰
    from backend.config import reset_config
    from backend.services.checklist_service import reset_checklist_service
    from backend.services.llm_provider import reset_llm_provider
    from backend.services.memory import reset_memory_service
    from backend.services.memory_refiner import reset_memory_refiner
    from backend.services.search import reset_search_service

    reset_config()
    reset_search_service()
    reset_memory_service()
    reset_checklist_service()
    reset_llm_provider()
    reset_memory_refiner()

    async with stdio_server() as (read_stream, write_stream):
        await server.run(
            read_stream,
            write_stream,
            server.create_initialization_options(),
        )


if __name__ == "__main__":
    asyncio.run(main())
